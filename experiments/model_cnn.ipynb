{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d48769c5-a167-4706-9639-50aa64fe0e6f",
   "metadata": {},
   "source": [
    "# header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4279f410-2517-4234-ade1-768976367c03",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import datetime\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from numba import cuda\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "import config as CONFIG\n",
    "import utils\n",
    "\n",
    "import wandb\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset, SubsetRandomSampler\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f916e65-2402-4c07-8747-1a6ba7a2920d",
   "metadata": {},
   "source": [
    "# generate features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7a58be63-e1e5-40ac-a692-2f34d8007475",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# single process, go \"feats.py\" for multi process\n",
    "max_row = 131072\n",
    "max_col = 1024\n",
    "row_block = 32\n",
    "col_block = 16\n",
    "row_section_size = max_row // row_block\n",
    "col_section_size = max_col // col_block\n",
    "\n",
    "def process_one(i):\n",
    "    df = pd.read_csv(os.path.join(CONFIG.PATH_PROCESSED, f\"mcelog_{i}.csv\"))\n",
    "    data = []\n",
    "    labels = []\n",
    "    hosts = df[\"sid\"].drop_duplicates().to_list()\n",
    "\n",
    "    for host in tqdm(hosts):\n",
    "        host_matrix = np.zeros([row_block, col_block])\n",
    "        host_df = df[df[\"sid\"]==host]\n",
    "        host_df = host_df.fillna(-1)\n",
    "        \n",
    "        label = (int(host_df[\"failure_type\"].sum() < 0) + 1) % 2\n",
    "        labels.append(label)\n",
    "\n",
    "        host_df[\"row_section\"] = host_df[\"row\"] // row_section_size\n",
    "        host_df[\"col_section\"] = host_df[\"col\"] // col_section_size\n",
    "\n",
    "        host_df_simple = host_df[[\"row_section\", \"col_section\"]].drop_duplicates()\n",
    "\n",
    "        for _, row in host_df_simple.iterrows():\n",
    "            host_matrix[int(row[\"row_section\"]), int(row[\"col_section\"])] = 1\n",
    "        data.append(host_matrix)\n",
    "    \n",
    "    np.save(os.path.join(CONFIG.PATH_PROCESSED, f\"feats_{i}.npy\"), data)\n",
    "    np.save(os.path.join(CONFIG.PATH_PROCESSED, f\"labels_{i}.npy\"), labels)\n",
    "    \n",
    "    \n",
    "# for i in range(0, 31):\n",
    "#     if i == 22:\n",
    "#         continue\n",
    "#     process_one(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47a0222-9191-4d9c-8f15-a1817ced8570",
   "metadata": {},
   "source": [
    "# load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8f7f747c-4472-4aa2-ade0-a1405eb3f027",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0. , 0. , 0. ],\n",
       "       [0. , 2.2, 0. ],\n",
       "       [0. , 0. , 0. ]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "host_matrix = np.zeros([3, 3])\n",
    "\n",
    "host_matrix[1,1] += 1\n",
    "host_matrix[1,1] += 1.2\n",
    "\n",
    "host_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e9635899-78b8-4ee0-b238-59fdfd738ca1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "dataset_list = []\n",
    "\n",
    "\n",
    "for i in range(31):\n",
    "    if i == 22:\n",
    "        continue\n",
    "    data_np = np.load(os.path.join(CONFIG.PATH_PROCESSED, f\"feats_64x32_with_times_{i}.npy\"))\n",
    "    label_np = np.load(os.path.join(CONFIG.PATH_PROCESSED, f\"labels_64x32_{i}.npy\"))\n",
    "    \n",
    "    data_np = 2 / (1 + np.exp(-data_np)) - 1\n",
    "\n",
    "    data = torch.tensor(data_np, dtype=torch.float32)\n",
    "    label = torch.tensor(label_np, dtype=torch.int64)\n",
    "     \n",
    "    dataset = TensorDataset(data, label)\n",
    "    dataset_list.append(dataset)\n",
    "\n",
    "datasets = torch.utils.data.ConcatDataset(dataset_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "88336636-7047-4564-9336-d1378f9b1221",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_np[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc73cd8-13e5-43f3-879d-d8f0804f459b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## check ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b472e630-b726-4950-85e3-a0b3fb5762bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1429df70-0e52-4d73-9e94-7846a59196ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "dataset_list = []\n",
    "\n",
    "xgb_data_list = []\n",
    "xgb_label_list = []\n",
    "\n",
    "cnt = 0\n",
    "error = 0\n",
    "enrich_ratio = 1\n",
    "under_sampling_ratio = 8\n",
    "\n",
    "for i in range(31):\n",
    "    if i == 22:\n",
    "        continue\n",
    "    data_np = np.load(os.path.join(CONFIG.PATH_PROCESSED, f\"feats_64x32_with_times_{i}.npy\"))\n",
    "    label_np = np.load(os.path.join(CONFIG.PATH_PROCESSED, f\"labels_64x32_{i}.npy\"))\n",
    "    \n",
    "    data_list = data_np.tolist()\n",
    "    label_list = label_np.tolist()\n",
    "    \n",
    "    tmp_data_list = []\n",
    "    tmp_label_list = []\n",
    "    for idx in range(len(data_list)):\n",
    "        curdata = data_list[idx]\n",
    "        curlabel = label_list[idx]\n",
    "        if curlabel == 1:\n",
    "            for _ in range(enrich_ratio):\n",
    "                tmp_data_list.append(curdata)\n",
    "                tmp_label_list.append(curlabel)\n",
    "        else:\n",
    "            if idx % under_sampling_ratio == 0:\n",
    "                tmp_data_list.append(curdata)\n",
    "                tmp_label_list.append(curlabel)\n",
    "    cnt += len(label_list)   \n",
    "    error += sum(label_list)\n",
    "    \n",
    "    \n",
    "    data_list = tmp_data_list\n",
    "    label_list = tmp_label_list\n",
    "    \n",
    "    xgb_data_list.extend(data_list)\n",
    "    xgb_label_list.extend(label_list)\n",
    "    \n",
    "    data_np = np.array(data_list)\n",
    "    label_np = np.array(label_list)\n",
    "    \n",
    "    data_np = 2 / (1 + np.exp(-data_np)) - 1\n",
    "    \n",
    "    data = torch.tensor(data_np, dtype=torch.float32)\n",
    "    label = torch.tensor(label_np, dtype=torch.int64)\n",
    "     \n",
    "    dataset = TensorDataset(data, label)\n",
    "    dataset_list.append(dataset)\n",
    "\n",
    "datasets = torch.utils.data.ConcatDataset(dataset_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5934d3cc-6ee7-4a3b-9f44-e109ccf732b4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4296"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(xgb_data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6f422c94-ed4c-4bf8-9615-c3c0973c97ab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "566"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(xgb_label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ca6cce61-c310-4382-a82e-60184db6c00c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 64, 32])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1838638d-beec-4716-849f-bb4a63ab54cf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1, 64, 32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d674efc4-3177-40d5-be59-be3572dd004c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'len'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdatasets\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlen\u001b[49m()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'len'"
     ]
    }
   ],
   "source": [
    "datasets[0].len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "765656a5-d48a-4c09-bd48-9f439c759d60",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ret_data = None\n",
    "ret_label = None\n",
    "\n",
    "for i in range(31):\n",
    "    if i == 22:\n",
    "        continue\n",
    "    data_np = np.load(os.path.join(CONFIG.PATH_PROCESSED, f\"feats_32x16_with_times_{i}.npy\"))\n",
    "    label_np = np.load(os.path.join(CONFIG.PATH_PROCESSED, f\"labels_32x16_{i}.npy\"))\n",
    "    if ret_data is None:\n",
    "        ret_data = data_np\n",
    "        ret_label = label_np\n",
    "    else:\n",
    "        ret_data = np.concatenate((ret_data, data_np))\n",
    "        ret_label = np.concatenate((ret_label, label_np))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d690b940-248d-4ab2-9e94-e8a19b4cd5eb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30264, 1, 32, 16)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6bab4444-db40-445d-9a16-d36833286964",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30264,)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "44aad297-117d-4d7e-85f7-8a84af4270b8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret_label[256]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "950a219a-927b-4729-81f8-c00ad15fa6bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.save(os.path.join(CONFIG.PATH_PROCESSED, \"feats_32x16_with_times.npy\"), ret_data)\n",
    "np.save(os.path.join(CONFIG.PATH_PROCESSED, \"labels_32x16.npy\"), ret_label)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "4b9f2107-46d5-4ee0-bf2a-93e0b72c0102",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ddd = np.load(os.path.join(CONFIG.PATH_PROCESSED, \"feats_32x16_with_times.npy\"))\n",
    "lll = np.load(os.path.join(CONFIG.PATH_PROCESSED, \"labels_32x16.npy\"))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "b3fdb465-fdf9-41f2-be23-53852b1a8b57",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30264, 1, 32, 16)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "db4b9cc0-c5af-40c8-8945-547eb4ae87e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ddd = np.load(os.path.join(CONFIG.PATH_PROCESSED, \"feats_64x32_with_times.npy\"))\n",
    "lll = np.load(os.path.join(CONFIG.PATH_PROCESSED, \"labels_64x32.npy\"))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "b86d53a9-5fbb-450d-b71e-7cb6793ceae8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30264, 1, 64, 32)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "4c134258-155d-4e6d-8396-dc6b224b91cf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddd[345].any()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b860fc66-21c3-49eb-8976-31b05e398fde",
   "metadata": {
    "tags": []
   },
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf7ef29-f373-478a-a8fa-210afae690b6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## single testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "29839fcd-e477-445a-a184-011ec66dc076",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前设备: cuda\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected input batch_size (256) to match target batch_size (64).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[56], line 296\u001b[0m\n\u001b[1;32m    294\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m    295\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(_inputs)  \u001b[38;5;66;03m# 在通道维度上增加1\u001b[39;00m\n\u001b[0;32m--> 296\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_labels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    297\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m    298\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/anaconda3/envs/dram-py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/dram-py310/lib/python3.10/site-packages/torch/nn/modules/loss.py:1174\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1173\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m-> 1174\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1175\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1176\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/dram-py310/lib/python3.10/site-packages/torch/nn/functional.py:3029\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3027\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3028\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3029\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected input batch_size (256) to match target batch_size (64)."
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "# class CNNModel(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(CNNModel, self).__init__()\n",
    "#         self.channels = 64\n",
    "        \n",
    "#         self.conv1 = nn.Conv2d(1, self.channels, kernel_size=9, padding=4)\n",
    "#         self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "#         self.fc1 = nn.Linear(self.channels * 16 * 8, 128)  \n",
    "#         self.dropout = nn.Dropout(0.1)\n",
    "#         self.fc2 = nn.Linear(128, 2)  \n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.pool(F.relu(self.conv1(x)))\n",
    "#         x = x.view(-1, self.channels * 16 * 8)\n",
    "#         x = F.relu(self.fc1(x))\n",
    "#         x = self.dropout(x)\n",
    "#         x = self.fc2(x)\n",
    "#         return x\n",
    "\n",
    "# class CNNModel(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(CNNModel, self).__init__()\n",
    "#         self.channels1 = 32\n",
    "#         self.channels2 = 64\n",
    "        \n",
    "#         self.conv1 = nn.Conv2d(1, self.channels1, kernel_size=3, padding=1)\n",
    "#         self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "#         self.conv2 = nn.Conv2d(self.channels1, self.channels2, kernel_size=3, padding=1)\n",
    "#         self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "#         self.fc1 = nn.Linear(self.channels2 * 7 * 7, 128)\n",
    "        \n",
    "#         self.fc2 = nn.Linear(128, 2)  # 输出2类，根据你的任务调整输出维度\n",
    "\n",
    "#         self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.pool1(F.relu(self.conv1(x)))\n",
    "#         x = self.pool2(F.relu(self.conv2(x)))\n",
    "#         x = x.view(-1, self.channels2 * 7 * 7)\n",
    "#         x = F.relu(self.fc1(x))\n",
    "#         x = self.dropout(x)\n",
    "#         x = self.fc2(x)\n",
    "#         return x\n",
    "\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNModel, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=9, padding=4)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=7, padding=3)\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=5, padding=2)\n",
    "        self.conv4 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1)\n",
    "        \n",
    "        # self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1)\n",
    "        # self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        # self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
    "        # self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
    "        # self.conv4 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1)\n",
    "        \n",
    "        # 全连接层\n",
    "        self.fc1 = nn.Linear(256 * 2 * 1, 256) \n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.fc2 = nn.Linear(256, 2)  \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = self.pool(F.relu(self.conv4(x)))\n",
    "        x = x.view(-1, 256 * 2 * 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "########################################################################\n",
    "# 定义基本的ResNet块\n",
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=9, stride=stride, padding=4, bias=False)\n",
    "        # self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        # self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.downsample = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.downsample = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.conv1(x)\n",
    "        # out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        # out = self.bn2(out)\n",
    "        if self.downsample:\n",
    "            identity = self.downsample(x)\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "# 定义ResNet网络\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=2):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_channels = 64\n",
    "        self.conv1 = nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        # self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(256, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, out_channels, num_blocks, stride):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_channels, out_channels, stride))\n",
    "            self.in_channels = out_channels\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        # x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# 创建ResNet模型\n",
    "def ResNetCustom():\n",
    "    return ResNet(BasicBlock, [2, 2, 2])  # 这里定义了一个3层的ResNet\n",
    "########################################################################\n",
    "\n",
    "# wandb.init(\n",
    "#     project=\"dram\",\n",
    "    \n",
    "#     config={\n",
    "\n",
    "#     }\n",
    "# )\n",
    "    \n",
    "    \n",
    "\n",
    "batch_size = 64\n",
    "data_loader = DataLoader(datasets, shuffle=True)\n",
    "\n",
    "\n",
    "train_ratio = 0.8\n",
    "test_ratio = 1 - train_ratio  \n",
    "\n",
    "total_samples = len(data_loader.dataset)\n",
    "\n",
    "train_size = int(train_ratio * total_samples)\n",
    "test_size = total_samples - train_size\n",
    "\n",
    "indices = list(range(total_samples))\n",
    "\n",
    "random.shuffle(indices)\n",
    "\n",
    "train_indices = indices[:train_size]\n",
    "test_indices = indices[train_size:]\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "test_sampler = SubsetRandomSampler(test_indices)\n",
    "\n",
    "train_loader = DataLoader(datasets, batch_size=batch_size, sampler=train_sampler)\n",
    "test_loader = DataLoader(datasets, batch_size=batch_size, sampler=test_sampler)\n",
    "\n",
    "\n",
    "############################################################################\n",
    "\n",
    "\n",
    "\n",
    "#######################################################################################\n",
    "\n",
    "# # model = CNNModel()\n",
    "# model = ResNetCustom()\n",
    "\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# print(\"当前设备:\", device)\n",
    "# model.to(device)\n",
    "\n",
    "# criterion = nn.CrossEntropyLoss() \n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.001) \n",
    "\n",
    "# num_epochs = 100\n",
    "# for epoch in range(num_epochs):\n",
    "#     running_loss = 0.0\n",
    "#     for _inputs, _labels in train_loader:\n",
    "#         _inputs, _labels = _inputs.to(device), _labels.to(device)\n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = model(_inputs.unsqueeze(1))  # 在通道维度上增加1\n",
    "#         loss = criterion(outputs, _labels)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         running_loss += loss.item()\n",
    "#         # wandb.log({\"loss\": running_loss})\n",
    "#     print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {running_loss / len(data_loader)}')\n",
    "\n",
    "# print('Finished Training')\n",
    "\n",
    "\n",
    "# model.eval()\n",
    "\n",
    "\n",
    "\n",
    "# predictions = []\n",
    "# true_labels = []\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     for _inputs, _labels in train_loader:\n",
    "#         _inputs, _labels = _inputs.to(device), _labels.to(device)\n",
    "#         outputs = model(_inputs.unsqueeze(1))\n",
    "#         _, predicted = torch.max(outputs.data, 1)\n",
    "#         predictions.extend(predicted.tolist())\n",
    "#         true_labels.extend(_labels.tolist())\n",
    "\n",
    "# precision = precision_score(true_labels, predictions)\n",
    "# recall = recall_score(true_labels, predictions)\n",
    "\n",
    "# print(f\"all: {len(true_labels)}, error_labels: {sum(true_labels)}\")\n",
    "# print(f'Precision: {precision}')\n",
    "# print(f'Recall: {recall}')\n",
    "\n",
    "# # wandb.log({\"train precision\": precision, \"train recall\": recall})\n",
    "\n",
    "\n",
    "# predictions = []\n",
    "# true_labels = []\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     for _inputs, _labels in test_loader:\n",
    "#         _inputs, _labels = _inputs.to(device), _labels.to(device)\n",
    "#         outputs = model(_inputs.unsqueeze(1))\n",
    "#         _, predicted = torch.max(outputs.data, 1)\n",
    "#         predictions.extend(predicted.tolist())\n",
    "#         true_labels.extend(_labels.tolist())\n",
    "\n",
    "# print(f\"all: {len(true_labels)}, error_labels: {sum(true_labels)}\")\n",
    "        \n",
    "# precision = precision_score(true_labels, predictions)\n",
    "# recall = recall_score(true_labels, predictions)\n",
    "\n",
    "# print(f'Precision: {precision}')\n",
    "# print(f'Recall: {recall}')\n",
    "\n",
    "# # wandb.log({\"test precision\": precision, \"test recall\": recall})\n",
    "\n",
    "# # wandb.finish()\n",
    "#######################################################################################\n",
    "\n",
    "\n",
    "model = CNNModel()\n",
    "# wandb.watch(model)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"当前设备:\", device)\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss() \n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001) \n",
    "\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for _inputs, _labels in train_loader:\n",
    "        _inputs, _labels = _inputs.to(device), _labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(_inputs)  # 在通道维度上增加1\n",
    "        loss = criterion(outputs, _labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        # wandb.log({\"loss\": running_loss})\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {running_loss / len(data_loader)}')\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "\n",
    "model.eval()\n",
    "\n",
    "\n",
    "\n",
    "predictions = []\n",
    "true_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for _inputs, _labels in train_loader:\n",
    "        _inputs, _labels = _inputs.to(device), _labels.to(device)\n",
    "        outputs = model(_inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        predictions.extend(predicted.tolist())\n",
    "        true_labels.extend(_labels.tolist())\n",
    "\n",
    "precision = precision_score(true_labels, predictions)\n",
    "recall = recall_score(true_labels, predictions)\n",
    "\n",
    "print(f\"all: {len(true_labels)}, error_labels: {sum(true_labels)}\")\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall: {recall}')\n",
    "\n",
    "# wandb.log({\"train precision\": precision, \"train recall\": recall})\n",
    "\n",
    "\n",
    "predictions = []\n",
    "true_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for _inputs, _labels in test_loader:\n",
    "        _inputs, _labels = _inputs.to(device), _labels.to(device)\n",
    "        outputs = model(_inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        predictions.extend(predicted.tolist())\n",
    "        true_labels.extend(_labels.tolist())\n",
    "\n",
    "print(f\"all: {len(true_labels)}, error_labels: {sum(true_labels)}\")\n",
    "        \n",
    "precision = precision_score(true_labels, predictions)\n",
    "recall = recall_score(true_labels, predictions)\n",
    "\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall: {recall}')\n",
    "\n",
    "# wandb.log({\"test precision\": precision, \"test recall\": recall})\n",
    "\n",
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b98399-9f88-4aff-a7ea-162f0a95a679",
   "metadata": {
    "tags": []
   },
   "source": [
    "## lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "10a4d39a-362d-4b9e-967d-53f0073d89f1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 447, number of negative: 2989\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 1.498166 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3610\n",
      "[LightGBM] [Info] Number of data points in the train set: 3436, number of used features: 470\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.130093 -> initscore=-1.900136\n",
      "[LightGBM] [Info] Start training from score -1.900136\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Accuracy: 81.28%\n",
      "Precision: 0.31896551724137934\n",
      "Recall: 0.31092436974789917\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 假设你有N个样本，每个样本是一个3x2的矩阵\n",
    "# N = 1000\n",
    "data = np.array(xgb_data_list)\n",
    "labels = np.array(xgb_label_list)\n",
    "\n",
    "# 将数据整形为(N, 6)，以便传递给LightGBM\n",
    "data = data.reshape(len(data), -1)\n",
    "\n",
    "# 划分训练集和测试集\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# 创建LightGBM数据集\n",
    "train_data = lgb.Dataset(X_train, label=y_train)\n",
    "test_data = lgb.Dataset(X_test, label=y_test, reference=train_data)\n",
    "\n",
    "# 定义LightGBM参数\n",
    "params = {\n",
    "    'objective': 'binary',  # 二分类问题\n",
    "    'metric': 'binary_error',  # 评估指标为二分类错误率\n",
    "    'boosting_type': 'gbdt',  # 使用梯度提升树\n",
    "    'num_leaves': 128,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.3\n",
    "}\n",
    "\n",
    "# 训练LightGBM模型\n",
    "num_round = 100  # 迭代次数\n",
    "bst = lgb.train(params, train_data, num_round, valid_sets=[test_data])\n",
    "\n",
    "# 预测\n",
    "y_pred = bst.predict(X_test, num_iteration=bst.best_iteration)\n",
    "y_pred_binary = [1 if x >= 0.2 else 0 for x in y_pred]\n",
    "\n",
    "# 评估模型\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred_binary)\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
    "\n",
    "precision = precision_score(y_test, y_pred_binary)\n",
    "recall = recall_score(y_test, y_pred_binary)\n",
    "\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall: {recall}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e215bd95-43c6-4614-aa94-e4c639ba4fbe",
   "metadata": {},
   "source": [
    "## pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "87e173cf-e4aa-444b-9bca-316af168e64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ptn_data = np.load(os.path.join(CONFIG.PATH_PROCESSED, f\"feats_with_times_{0}.npy\"))\n",
    "ptn_label = np.load(os.path.join(CONFIG.PATH_PROCESSED, f\"labels_{0}.npy\"))\n",
    "\n",
    "ptn_data = 2/(1+np.exp(-ptn_data))-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "bb01f190-4216-41f6-9f2e-cc7897404a6e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ptn_data[45]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39680c0-60ef-496c-a69f-322d135c5d3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c542d5e-8e7b-4bf8-ad72-991f8c4d1752",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9ee474ae-5f91-4c5f-bebd-cf39d7b2b6ee",
   "metadata": {},
   "source": [
    "## xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b3b9c888-a66f-47e5-973e-8b0745e3ea56",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'xgboost'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mxgboost\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mxgb\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m accuracy_score\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'xgboost'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 假设你有N个样本，每个样本是一个3x2的矩阵\n",
    "# N = 1000\n",
    "data = np.array(xgb_data_list)\n",
    "labels = np.array(xgb_label_list)\n",
    "\n",
    "# 将数据整形为(N, 6)，以便传递给LightGBM\n",
    "data = data.reshape(len(data), -1)\n",
    "\n",
    "# 划分训练集和测试集\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# model = xgb.XGBClassifier(max_depth=6,\n",
    "#     learning_rate=0.01,\n",
    "#     n_estimators=100,\n",
    "#     objective='binary:logistic',\n",
    "#     eval_metric='logloss',\n",
    "#     random_state=42)\n",
    "model = xgb.XGBClassifier(objective='binary:logistic')\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# 评估模型\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
    "\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall: {recall}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c4557c-4a99-4dd0-a46f-e6532163abc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d884d05c-99f0-4db1-a58a-75fc7ebe0ce9",
   "metadata": {},
   "source": [
    "## Random Forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "e23408d6-8daf-4c17-bc18-c91c9af4e3b6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 84.42%\n",
      "Precision: 0.3972602739726027\n",
      "Recall: 0.24369747899159663\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "data = np.array(xgb_data_list)\n",
    "labels = np.array(xgb_label_list)\n",
    "\n",
    "data = data.reshape(len(data), -1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=50, \n",
    "                            random_state=42,\n",
    "                            criterion=\"gini\")\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
    "\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall: {recall}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9782c7-c672-43c3-86a1-2277f84d4f1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a259ab01-c092-44e8-b48c-067730f987d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e00c1ab-26ab-49f6-909f-c9af2ffb36ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc67780-02d3-4850-ac92-3c5f686e97b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d6333bf5-c14c-42f1-9c7b-2b73f59bd78e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "d0fedff1-1192-4535-9693-4219da44fdf0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练准确率: 0.51\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 创建示例数据（请根据您的数据替换这部分代码）\n",
    "N = 1000\n",
    "data = np.random.rand(N, 1, 32, 16)\n",
    "labels = np.random.randint(0, 2, N)\n",
    "\n",
    "# 划分训练集和测试集\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# 将数据转换为 PyTorch 张量\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "# 创建数据加载器\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# 定义基本的ResNet块\n",
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.downsample = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.downsample = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        if self.downsample:\n",
    "            identity = self.downsample(x)\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "# 定义ResNet网络\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=2):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_channels = 64\n",
    "        self.conv1 = nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(256, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, out_channels, num_blocks, stride):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_channels, out_channels, stride))\n",
    "            self.in_channels = out_channels\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# 创建ResNet模型\n",
    "def ResNetCustom():\n",
    "    return ResNet(BasicBlock, [2, 2, 2])  # 这里定义了一个3层的ResNet\n",
    "\n",
    "# 训练模型\n",
    "model = ResNetCustom()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# 模型评估\n",
    "model.eval()\n",
    "y_pred = []\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        y_pred.extend(predicted.cpu().numpy())\n",
    "\n",
    "accuracy = accuracy_score(y_train, y_pred)\n",
    "print(f\"训练准确率: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c80a0bab-4be2-4f43-8cb6-225222a1c595",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.7166968778922007\n",
      "Epoch 2/10, Loss: 0.2823257944904841\n",
      "Epoch 3/10, Loss: 0.1604343234346463\n",
      "Epoch 4/10, Loss: 0.148019632181296\n",
      "Epoch 5/10, Loss: 0.13936221699875134\n",
      "Epoch 6/10, Loss: 0.11386105848046449\n",
      "Epoch 7/10, Loss: 0.10020001146655816\n",
      "Epoch 8/10, Loss: 0.09341820701956749\n",
      "Epoch 9/10, Loss: 0.0867923440841528\n",
      "Epoch 10/10, Loss: 0.0762054229585024\n",
      "Accuracy on test set: 95.22%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "\n",
    "# 生成模拟数据\n",
    "N = 1000\n",
    "data = np.random.randn(N, 1, 64, 32).astype(np.float32)\n",
    "labels = np.random.randint(0, 2, N)  # 生成随机标签\n",
    "data = np.load(os.path.join(CONFIG.PATH_PROCESSED, \"feats_64x32_with_times.npy\")).astype(np.float32)\n",
    "labels = np.load(os.path.join(CONFIG.PATH_PROCESSED, \"labels_64x32.npy\"))\n",
    "\n",
    "# 划分训练集和测试集\n",
    "split_ratio = 0.8\n",
    "split = int(N * split_ratio)\n",
    "train_data, test_data = data[:split], data[split:]\n",
    "train_labels, test_labels = labels[:split], labels[split:]\n",
    "\n",
    "# 创建自定义数据集类\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = {'data': self.data[idx], 'label': self.labels[idx]}\n",
    "        return sample\n",
    "\n",
    "# 创建数据集和数据加载器\n",
    "batch_size = 64\n",
    "train_dataset = CustomDataset(train_data, train_labels)\n",
    "test_dataset = CustomDataset(test_data, test_labels)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# 定义一个两层CNN模型\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc = nn.Linear(32 * 16 * 8, 2)  # 输出2个类别\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# 创建模型实例\n",
    "model = SimpleCNN()\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 训练模型\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for batch in train_loader:\n",
    "        inputs, labels = batch['data'], batch['label']\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {running_loss / len(train_loader)}')\n",
    "\n",
    "# 在测试集上验证模型\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        inputs, labels = batch['data'], batch['label']\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy on test set: {(correct / total) * 100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "dbba72ca-6add-4a9f-bdfa-4ee7c9eaba18",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-16 01:09:55,109 - INFO - Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg2_in21k_ft_in1k)\n"
     ]
    },
    {
     "ename": "LocalEntryNotFoundError",
     "evalue": "An error happened while trying to locate the file on the Hub and we cannot find the requested files in the local cache. Please check your connection and try again or make sure your Internet connection is on.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/dram-py310/lib/python3.10/site-packages/urllib3/connection.py:174\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 174\u001b[0m     conn \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_connection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dns_host\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mextra_kw\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SocketTimeout:\n",
      "File \u001b[0;32m~/anaconda3/envs/dram-py310/lib/python3.10/site-packages/urllib3/util/connection.py:95\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 95\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m socket\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgetaddrinfo returns an empty list\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/dram-py310/lib/python3.10/site-packages/urllib3/util/connection.py:85\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     84\u001b[0m     sock\u001b[38;5;241m.\u001b[39mbind(source_address)\n\u001b[0;32m---> 85\u001b[0m \u001b[43msock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43msa\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sock\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 101] Network is unreachable",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNewConnectionError\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/dram-py310/lib/python3.10/site-packages/urllib3/connectionpool.py:714\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    713\u001b[0m \u001b[38;5;66;03m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> 714\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    715\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    716\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    717\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    718\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    719\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    720\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    721\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    722\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    724\u001b[0m \u001b[38;5;66;03m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    725\u001b[0m \u001b[38;5;66;03m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    726\u001b[0m \u001b[38;5;66;03m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    727\u001b[0m \u001b[38;5;66;03m# mess.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/dram-py310/lib/python3.10/site-packages/urllib3/connectionpool.py:403\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    402\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 403\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    405\u001b[0m     \u001b[38;5;66;03m# Py2 raises this as a BaseSSLError, Py3 raises it as socket timeout.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/dram-py310/lib/python3.10/site-packages/urllib3/connectionpool.py:1053\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m   1052\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(conn, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msock\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):  \u001b[38;5;66;03m# AppEngine might not have  `.sock`\u001b[39;00m\n\u001b[0;32m-> 1053\u001b[0m     \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1055\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_verified:\n",
      "File \u001b[0;32m~/anaconda3/envs/dram-py310/lib/python3.10/site-packages/urllib3/connection.py:363\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconnect\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;66;03m# Add certificate verification\u001b[39;00m\n\u001b[0;32m--> 363\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_new_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    364\u001b[0m     hostname \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost\n",
      "File \u001b[0;32m~/anaconda3/envs/dram-py310/lib/python3.10/site-packages/urllib3/connection.py:186\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SocketError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 186\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NewConnectionError(\n\u001b[1;32m    187\u001b[0m         \u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to establish a new connection: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m e\n\u001b[1;32m    188\u001b[0m     )\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m conn\n",
      "\u001b[0;31mNewConnectionError\u001b[0m: <urllib3.connection.HTTPSConnection object at 0x7f52edd16f80>: Failed to establish a new connection: [Errno 101] Network is unreachable",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/dram-py310/lib/python3.10/site-packages/requests/adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 486\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    487\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/anaconda3/envs/dram-py310/lib/python3.10/site-packages/urllib3/connectionpool.py:798\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    796\u001b[0m     e \u001b[38;5;241m=\u001b[39m ProtocolError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection aborted.\u001b[39m\u001b[38;5;124m\"\u001b[39m, e)\n\u001b[0;32m--> 798\u001b[0m retries \u001b[38;5;241m=\u001b[39m \u001b[43mretries\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mincrement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    801\u001b[0m retries\u001b[38;5;241m.\u001b[39msleep()\n",
      "File \u001b[0;32m~/anaconda3/envs/dram-py310/lib/python3.10/site-packages/urllib3/util/retry.py:592\u001b[0m, in \u001b[0;36mRetry.increment\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m new_retry\u001b[38;5;241m.\u001b[39mis_exhausted():\n\u001b[0;32m--> 592\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MaxRetryError(_pool, url, error \u001b[38;5;129;01mor\u001b[39;00m ResponseError(cause))\n\u001b[1;32m    594\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncremented Retry for (url=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, url, new_retry)\n",
      "\u001b[0;31mMaxRetryError\u001b[0m: HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /timm/vit_base_patch16_224.augreg2_in21k_ft_in1k/resolve/main/pytorch_model.bin (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f52edd16f80>: Failed to establish a new connection: [Errno 101] Network is unreachable'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/dram-py310/lib/python3.10/site-packages/huggingface_hub/file_download.py:1230\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, endpoint, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout)\u001b[0m\n\u001b[1;32m   1229\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1230\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m \u001b[43mget_hf_file_metadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1231\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1232\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1233\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1234\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1235\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1236\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m http_error:\n\u001b[1;32m   1237\u001b[0m     \u001b[38;5;66;03m# Cache the non-existence of the file and raise\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/dram-py310/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    116\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 118\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/dram-py310/lib/python3.10/site-packages/huggingface_hub/file_download.py:1597\u001b[0m, in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout)\u001b[0m\n\u001b[1;32m   1596\u001b[0m \u001b[38;5;66;03m# Retrieve metadata\u001b[39;00m\n\u001b[0;32m-> 1597\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1598\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHEAD\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1599\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1600\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1601\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1602\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1603\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1604\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1605\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1606\u001b[0m hf_raise_for_status(r)\n",
      "File \u001b[0;32m~/anaconda3/envs/dram-py310/lib/python3.10/site-packages/huggingface_hub/file_download.py:417\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, max_retries, base_wait_time, max_wait_time, timeout, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m follow_relative_redirects:\n\u001b[0;32m--> 417\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    418\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    419\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    420\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    421\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbase_wait_time\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_wait_time\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    422\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_wait_time\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_wait_time\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    423\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    424\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    425\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    426\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    428\u001b[0m     \u001b[38;5;66;03m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[1;32m    429\u001b[0m     \u001b[38;5;66;03m# This is useful in case of a renamed repository.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/dram-py310/lib/python3.10/site-packages/huggingface_hub/file_download.py:452\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, max_retries, base_wait_time, max_wait_time, timeout, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    451\u001b[0m \u001b[38;5;66;03m# 3. Exponential backoff\u001b[39;00m\n\u001b[0;32m--> 452\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mhttp_backoff\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    454\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    456\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_wait_time\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_wait_time\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_wait_time\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_wait_time\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    458\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretry_on_exceptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mTimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mProxyError\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretry_on_status_codes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    461\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    462\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/dram-py310/lib/python3.10/site-packages/huggingface_hub/utils/_http.py:258\u001b[0m, in \u001b[0;36mhttp_backoff\u001b[0;34m(method, url, max_retries, base_wait_time, max_wait_time, retry_on_exceptions, retry_on_status_codes, **kwargs)\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;66;03m# Perform request and return if status_code is not in the retry list.\u001b[39;00m\n\u001b[0;32m--> 258\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m retry_on_status_codes:\n",
      "File \u001b[0;32m~/anaconda3/envs/dram-py310/lib/python3.10/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/anaconda3/envs/dram-py310/lib/python3.10/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/dram-py310/lib/python3.10/site-packages/huggingface_hub/utils/_http.py:63\u001b[0m, in \u001b[0;36mUniqueRequestIdAdapter.send\u001b[0;34m(self, request, *args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 63\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mRequestException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/envs/dram-py310/lib/python3.10/site-packages/requests/adapters.py:519\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    517\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m SSLError(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[0;32m--> 519\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[1;32m    521\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ClosedPoolError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mConnectionError\u001b[0m: (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /timm/vit_base_patch16_224.augreg2_in21k_ft_in1k/resolve/main/pytorch_model.bin (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f52edd16f80>: Failed to establish a new connection: [Errno 101] Network is unreachable'))\"), '(Request ID: f644debb-5763-4cc3-998b-1999b7d4846d)')",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mLocalEntryNotFoundError\u001b[0m                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[67], line 44\u001b[0m\n\u001b[1;32m     41\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvit_base_patch16_224\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# 选择 ViT 模型架构\u001b[39;00m\n\u001b[1;32m     42\u001b[0m num_classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m  \u001b[38;5;66;03m# 你的分类任务中的类别数\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mViTClassifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[1;32m     46\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)\n",
      "Cell \u001b[0;32mIn[67], line 26\u001b[0m, in \u001b[0;36mViTClassifier.__init__\u001b[0;34m(self, model_name, num_classes)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, model_name, num_classes):\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28msuper\u001b[39m(ViTClassifier, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m---> 26\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[43mtimm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpretrained\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mhead \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mhead\u001b[38;5;241m.\u001b[39min_features, num_classes)\n",
      "File \u001b[0;32m~/anaconda3/envs/dram-py310/lib/python3.10/site-packages/timm/models/_factory.py:114\u001b[0m, in \u001b[0;36mcreate_model\u001b[0;34m(model_name, pretrained, pretrained_cfg, pretrained_cfg_overlay, checkpoint_path, scriptable, exportable, no_jit, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m create_fn \u001b[38;5;241m=\u001b[39m model_entrypoint(model_name)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m set_layer_config(scriptable\u001b[38;5;241m=\u001b[39mscriptable, exportable\u001b[38;5;241m=\u001b[39mexportable, no_jit\u001b[38;5;241m=\u001b[39mno_jit):\n\u001b[0;32m--> 114\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_fn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpretrained\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_cfg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpretrained_cfg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_cfg_overlay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpretrained_cfg_overlay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m checkpoint_path:\n\u001b[1;32m    122\u001b[0m     load_checkpoint(model, checkpoint_path)\n",
      "File \u001b[0;32m~/anaconda3/envs/dram-py310/lib/python3.10/site-packages/timm/models/vision_transformer.py:1608\u001b[0m, in \u001b[0;36mvit_base_patch16_224\u001b[0;34m(pretrained, **kwargs)\u001b[0m\n\u001b[1;32m   1604\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\" ViT-Base (ViT-B/16) from original paper (https://arxiv.org/abs/2010.11929).\u001b[39;00m\n\u001b[1;32m   1605\u001b[0m \u001b[38;5;124;03mImageNet-1k weights fine-tuned from in21k @ 224x224, source https://github.com/google-research/vision_transformer.\u001b[39;00m\n\u001b[1;32m   1606\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1607\u001b[0m model_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(patch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m, embed_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m768\u001b[39m, depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m12\u001b[39m, num_heads\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m12\u001b[39m)\n\u001b[0;32m-> 1608\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43m_create_vision_transformer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mvit_base_patch16_224\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpretrained\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpretrained\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1609\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/anaconda3/envs/dram-py310/lib/python3.10/site-packages/timm/models/vision_transformer.py:1510\u001b[0m, in \u001b[0;36m_create_vision_transformer\u001b[0;34m(variant, pretrained, **kwargs)\u001b[0m\n\u001b[1;32m   1507\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1508\u001b[0m     _filter_fn \u001b[38;5;241m=\u001b[39m checkpoint_filter_fn\n\u001b[0;32m-> 1510\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbuild_model_with_cfg\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1511\u001b[0m \u001b[43m    \u001b[49m\u001b[43mVisionTransformer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1512\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvariant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1513\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1514\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained_filter_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_filter_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1515\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1516\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/dram-py310/lib/python3.10/site-packages/timm/models/_builder.py:393\u001b[0m, in \u001b[0;36mbuild_model_with_cfg\u001b[0;34m(model_cls, variant, pretrained, pretrained_cfg, pretrained_cfg_overlay, model_cfg, feature_cfg, pretrained_strict, pretrained_filter_fn, kwargs_filter, **kwargs)\u001b[0m\n\u001b[1;32m    391\u001b[0m num_classes_pretrained \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m features \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(model, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_classes\u001b[39m\u001b[38;5;124m'\u001b[39m, kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_classes\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m1000\u001b[39m))\n\u001b[1;32m    392\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pretrained:\n\u001b[0;32m--> 393\u001b[0m     \u001b[43mload_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    394\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    395\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_cfg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpretrained_cfg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    396\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_classes_pretrained\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    397\u001b[0m \u001b[43m        \u001b[49m\u001b[43min_chans\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43min_chans\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    398\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilter_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpretrained_filter_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpretrained_strict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[38;5;66;03m# Wrap the model in a feature extraction module if enabled\u001b[39;00m\n\u001b[1;32m    403\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m features:\n",
      "File \u001b[0;32m~/anaconda3/envs/dram-py310/lib/python3.10/site-packages/timm/models/_builder.py:186\u001b[0m, in \u001b[0;36mload_pretrained\u001b[0;34m(model, pretrained_cfg, num_classes, in_chans, filter_fn, strict)\u001b[0m\n\u001b[1;32m    184\u001b[0m         state_dict \u001b[38;5;241m=\u001b[39m load_state_dict_from_hf(\u001b[38;5;241m*\u001b[39mpretrained_loc)\n\u001b[1;32m    185\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 186\u001b[0m         state_dict \u001b[38;5;241m=\u001b[39m \u001b[43mload_state_dict_from_hf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_loc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    188\u001b[0m     model_name \u001b[38;5;241m=\u001b[39m pretrained_cfg\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124marchitecture\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthis model\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/dram-py310/lib/python3.10/site-packages/timm/models/_hub.py:188\u001b[0m, in \u001b[0;36mload_state_dict_from_hf\u001b[0;34m(model_id, filename)\u001b[0m\n\u001b[1;32m    185\u001b[0m             \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;66;03m# Otherwise, load using pytorch.load\u001b[39;00m\n\u001b[0;32m--> 188\u001b[0m cached_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhf_model_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_revision\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    189\u001b[0m _logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] Safe alternative not found for \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Loading weights using default pytorch.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mload(cached_file, map_location\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/dram-py310/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    116\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 118\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/dram-py310/lib/python3.10/site-packages/huggingface_hub/file_download.py:1347\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, endpoint, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout)\u001b[0m\n\u001b[1;32m   1344\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m head_call_error\n\u001b[1;32m   1345\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1346\u001b[0m         \u001b[38;5;66;03m# Otherwise: most likely a connection issue or Hub downtime => let's warn the user\u001b[39;00m\n\u001b[0;32m-> 1347\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LocalEntryNotFoundError(\n\u001b[1;32m   1348\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error happened while trying to locate the file on the Hub and we cannot find the requested files\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1349\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m in the local cache. Please check your connection and try again or make sure your Internet connection\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1350\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is on.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1351\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhead_call_error\u001b[39;00m\n\u001b[1;32m   1353\u001b[0m \u001b[38;5;66;03m# From now on, etag and commit_hash are not None.\u001b[39;00m\n\u001b[1;32m   1354\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m etag \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124metag must have been retrieved from server\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mLocalEntryNotFoundError\u001b[0m: An error happened while trying to locate the file on the Hub and we cannot find the requested files in the local cache. Please check your connection and try again or make sure your Internet connection is on."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as transforms\n",
    "import timm\n",
    "\n",
    "# 步骤1: 生成数据集\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, num_samples):\n",
    "        self.num_samples = num_samples\n",
    "        self.data = torch.randn(num_samples, 1, 32, 16)  # 随机生成示例数据\n",
    "        self.labels = torch.randint(0, 2, (num_samples,))  # 随机生成示例标签 (0或1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n",
    "\n",
    "# 步骤2: 定义 ViT 模型\n",
    "class ViTClassifier(nn.Module):\n",
    "    def __init__(self, model_name, num_classes):\n",
    "        super(ViTClassifier, self).__init__()\n",
    "\n",
    "        self.model = timm.create_model(model_name, pretrained=True)\n",
    "        self.model.head = nn.Linear(self.model.head.in_features, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "\n",
    "# 步骤3: 创建数据加载器\n",
    "num_samples = 1000  # 你的数据集大小\n",
    "batch_size = 32\n",
    "\n",
    "dataset = CustomDataset(num_samples)\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# 步骤4: 定义损失函数和优化器\n",
    "model_name = 'vit_base_patch16_224'  # 选择 ViT 模型架构\n",
    "num_classes = 2  # 你的分类任务中的类别数\n",
    "\n",
    "model = ViTClassifier(model_name, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 步骤5: 训练模型\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {running_loss / len(train_loader)}\")\n",
    "\n",
    "print(\"Training complete\")\n",
    "\n",
    "# 步骤6: 进行推理\n",
    "# 为了进行推理，可以加载保存的模型权重并使用模型进行分类预测\n",
    "\n",
    "# 保存模型权重\n",
    "torch.save(model.state_dict(), 'vit_model.pth')\n",
    "\n",
    "# 加载模型权重\n",
    "model.load_state_dict(torch.load('vit_model.pth'))\n",
    "model.eval()  # 设置模型为评估模式\n",
    "\n",
    "# 准备一个示例输入（假设你有一个图像 tensor）\n",
    "input_image = torch.randn(1, 1, 32, 16)  # 示例输入\n",
    "\n",
    "# 使用模型进行图像分类预测\n",
    "with torch.no_grad():\n",
    "    output = model(input_image)\n",
    "\n",
    "# 输出的概率分布用于分类预测\n",
    "predicted_class = torch.argmax(output, dim=1)\n",
    "print(\"Predicted Class:\", predicted_class.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "78acff11-75bc-4588-aa1c-7f08f9c989d1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[71], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ViTFeatureExtractor, ViTForImageClassification, ViTConfig\n\u001b[1;32m      3\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgoogle/vit-base-patch16-224-in21k\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# 选择 ViT 模型名称\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# 获取模型配置（不下载权重）\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'transformers'"
     ]
    }
   ],
   "source": [
    "from transformers import ViTFeatureExtractor, ViTForImageClassification, ViTConfig\n",
    "\n",
    "model_name = \"google/vit-base-patch16-224-in21k\"  # 选择 ViT 模型名称\n",
    "\n",
    "# 获取模型配置（不下载权重）\n",
    "model_config = ViTConfig.from_pretrained(model_name)\n",
    "\n",
    "# 打印模型配置\n",
    "print(model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1e21a239-fe62-48b0-980c-6c3871b32915",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://mirrors.aliyun.com/pypi/simple\n",
      "Requirement already satisfied: transformers in /home/anichikage/anaconda3/lib/python3.11/site-packages (4.29.2)\n",
      "Requirement already satisfied: filelock in /home/anichikage/anaconda3/lib/python3.11/site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /home/anichikage/anaconda3/lib/python3.11/site-packages (from transformers) (0.15.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/anichikage/anaconda3/lib/python3.11/site-packages (from transformers) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/anichikage/anaconda3/lib/python3.11/site-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/anichikage/anaconda3/lib/python3.11/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/anichikage/anaconda3/lib/python3.11/site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: requests in /home/anichikage/anaconda3/lib/python3.11/site-packages (from transformers) (2.29.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/anichikage/anaconda3/lib/python3.11/site-packages (from transformers) (0.13.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/anichikage/anaconda3/lib/python3.11/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: fsspec in /home/anichikage/anaconda3/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/anichikage/anaconda3/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.6.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/anichikage/anaconda3/lib/python3.11/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/anichikage/anaconda3/lib/python3.11/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/anichikage/anaconda3/lib/python3.11/site-packages (from requests->transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/anichikage/anaconda3/lib/python3.11/site-packages (from requests->transformers) (2023.5.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "8b45e771-238d-4574-a801-5bebb46e6dcd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-09-18 13:03:47,759] A new study created in memory with name: no-name-c65ffe4a-5346-4c2e-a97f-119b6a575c7a\n",
      "[W 2023-09-18 13:03:47,791] Trial 0 failed with parameters: {'conv1_kernel_size': 5, 'conv2_kernel_size': 2} because of the following error: ValueError('Expected input batch_size (21) to match target batch_size (32).').\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/anichikage/anaconda3/envs/dram-py310/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_2229641/2070025995.py\", line 79, in objective\n",
      "    train(model, train_loader, criterion, optimizer, num_epochs=5)\n",
      "  File \"/tmp/ipykernel_2229641/2070025995.py\", line 39, in train\n",
      "    loss = criterion(outputs, labels)\n",
      "  File \"/home/anichikage/anaconda3/envs/dram-py310/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/anichikage/anaconda3/envs/dram-py310/lib/python3.10/site-packages/torch/nn/modules/loss.py\", line 1174, in forward\n",
      "    return F.cross_entropy(input, target, weight=self.weight,\n",
      "  File \"/home/anichikage/anaconda3/envs/dram-py310/lib/python3.10/site-packages/torch/nn/functional.py\", line 3029, in cross_entropy\n",
      "    return torch._C._nn.cross_entropy_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index, label_smoothing)\n",
      "ValueError: Expected input batch_size (21) to match target batch_size (32).\n",
      "[W 2023-09-18 13:03:47,792] Trial 0 failed with value None.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected input batch_size (21) to match target batch_size (32).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[79], line 88\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     87\u001b[0m     study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 88\u001b[0m     \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# 运行10次试验\u001b[39;00m\n\u001b[1;32m     89\u001b[0m     best_params \u001b[38;5;241m=\u001b[39m study\u001b[38;5;241m.\u001b[39mbest_params\n\u001b[1;32m     90\u001b[0m     best_accuracy \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mstudy\u001b[38;5;241m.\u001b[39mbest_value\n",
      "File \u001b[0;32m~/anaconda3/envs/dram-py310/lib/python3.10/site-packages/optuna/study/study.py:442\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[1;32m    340\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    341\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    348\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    349\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    350\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    351\u001b[0m \n\u001b[1;32m    352\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    440\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    441\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 442\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    443\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    444\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    445\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    446\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    447\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    448\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    449\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    450\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    451\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    452\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/dram-py310/lib/python3.10/site-packages/optuna/study/_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 66\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     79\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/dram-py310/lib/python3.10/site-packages/optuna/study/_optimize.py:163\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 163\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m~/anaconda3/envs/dram-py310/lib/python3.10/site-packages/optuna/study/_optimize.py:251\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    247\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[1;32m    248\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    250\u001b[0m ):\n\u001b[0;32m--> 251\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[0;32m~/anaconda3/envs/dram-py310/lib/python3.10/site-packages/optuna/study/_optimize.py:200\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[1;32m    199\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 200\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    202\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    203\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[0;32mIn[79], line 79\u001b[0m, in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     76\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)\n\u001b[1;32m     78\u001b[0m \u001b[38;5;66;03m# 训练模型\u001b[39;00m\n\u001b[0;32m---> 79\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;66;03m# 评估模型\u001b[39;00m\n\u001b[1;32m     82\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m evaluate(model, val_loader)\n",
      "Cell \u001b[0;32mIn[79], line 39\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, dataloader, criterion, optimizer, num_epochs)\u001b[0m\n\u001b[1;32m     37\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     38\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[0;32m---> 39\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     41\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/anaconda3/envs/dram-py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/dram-py310/lib/python3.10/site-packages/torch/nn/modules/loss.py:1174\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1173\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m-> 1174\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1175\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1176\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/dram-py310/lib/python3.10/site-packages/torch/nn/functional.py:3029\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3027\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3028\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3029\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected input batch_size (21) to match target batch_size (32)."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import optuna\n",
    "import numpy as np\n",
    "\n",
    "# 定义数据生成器\n",
    "def generate_data(N):\n",
    "    data = np.random.rand(N, 1, 32, 16).astype(np.float32)\n",
    "    labels = np.random.randint(2, size=N)\n",
    "    return torch.tensor(data), torch.tensor(labels)\n",
    "\n",
    "# 定义CNN模型类\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self, num_classes, conv1_kernel_size, conv2_kernel_size):\n",
    "        super(CNNModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=conv1_kernel_size, padding=(conv1_kernel_size-1)//2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=conv2_kernel_size, padding=(conv2_kernel_size-1)//2)\n",
    "        self.fc1 = nn.Linear(64 * 8 * 4, 128)  # 根据数据大小进行调整\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.max_pool2d(x, 2)\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = torch.max_pool2d(x, 2)\n",
    "        x = x.view(-1, 64 * 8 * 4)  # 根据数据大小进行调整\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# 定义训练函数\n",
    "def train(model, dataloader, criterion, optimizer, num_epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        for inputs, labels in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "# 定义评估函数\n",
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    return correct / total\n",
    "\n",
    "# 定义Optuna试验函数\n",
    "def objective(trial):\n",
    "    # 定义超参数搜索范围\n",
    "    conv1_kernel_size = trial.suggest_int(\"conv1_kernel_size\", 3, 7)\n",
    "    conv2_kernel_size = trial.suggest_int(\"conv2_kernel_size\", 1, 3)\n",
    "    \n",
    "    # 创建数据加载器\n",
    "    train_data, train_labels = generate_data(1000)\n",
    "    train_dataset = torch.utils.data.TensorDataset(train_data, train_labels)\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    \n",
    "    val_data, val_labels = generate_data(200)\n",
    "    val_dataset = torch.utils.data.TensorDataset(val_data, val_labels)\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "    \n",
    "    # 创建模型\n",
    "    model = CNNModel(num_classes=2, conv1_kernel_size=conv1_kernel_size, conv2_kernel_size=conv2_kernel_size)\n",
    "    \n",
    "    # 定义损失函数和优化器\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    # 训练模型\n",
    "    train(model, train_loader, criterion, optimizer, num_epochs=5)\n",
    "    \n",
    "    # 评估模型\n",
    "    accuracy = evaluate(model, val_loader)\n",
    "    \n",
    "    return -accuracy  # 我们希望最大化召回率，因此返回负的准确度\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(objective, n_trials=10)  # 运行10次试验\n",
    "    best_params = study.best_params\n",
    "    best_accuracy = -study.best_value\n",
    "    print(f\"Best Parameters: {best_params}\")\n",
    "    print(f\"Best Accuracy: {best_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "011c0e8d-5fcb-4365-9dd1-2fa57d18671c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-09-18 13:07:31,039] A new study created in memory with name: no-name-d012f223-12b6-4bb8-81d5-64f2199e582a\n",
      "[I 2023-09-18 13:07:31,809] Trial 0 finished with value: -0.48 and parameters: {'conv1_kernel_size': 3, 'conv2_kernel_size': 1}. Best is trial 0 with value: -0.48.\n",
      "[I 2023-09-18 13:07:32,653] Trial 1 finished with value: -0.5 and parameters: {'conv1_kernel_size': 5, 'conv2_kernel_size': 3}. Best is trial 0 with value: -0.48.\n",
      "[I 2023-09-18 13:07:33,415] Trial 2 finished with value: -0.555 and parameters: {'conv1_kernel_size': 3, 'conv2_kernel_size': 1}. Best is trial 0 with value: -0.48.\n",
      "[I 2023-09-18 13:07:34,159] Trial 3 finished with value: -0.54 and parameters: {'conv1_kernel_size': 5, 'conv2_kernel_size': 1}. Best is trial 0 with value: -0.48.\n",
      "[I 2023-09-18 13:07:34,988] Trial 4 finished with value: -0.505 and parameters: {'conv1_kernel_size': 3, 'conv2_kernel_size': 3}. Best is trial 0 with value: -0.48.\n",
      "[I 2023-09-18 13:07:35,754] Trial 5 finished with value: -0.565 and parameters: {'conv1_kernel_size': 3, 'conv2_kernel_size': 1}. Best is trial 0 with value: -0.48.\n",
      "[I 2023-09-18 13:07:36,563] Trial 6 finished with value: -0.47 and parameters: {'conv1_kernel_size': 3, 'conv2_kernel_size': 3}. Best is trial 6 with value: -0.47.\n",
      "[I 2023-09-18 13:07:37,304] Trial 7 finished with value: -0.43 and parameters: {'conv1_kernel_size': 3, 'conv2_kernel_size': 1}. Best is trial 7 with value: -0.43.\n",
      "[I 2023-09-18 13:07:38,075] Trial 8 finished with value: -0.54 and parameters: {'conv1_kernel_size': 3, 'conv2_kernel_size': 1}. Best is trial 7 with value: -0.43.\n",
      "[I 2023-09-18 13:07:38,921] Trial 9 finished with value: -0.455 and parameters: {'conv1_kernel_size': 3, 'conv2_kernel_size': 3}. Best is trial 7 with value: -0.43.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'conv1_kernel_size': 3, 'conv2_kernel_size': 1}\n",
      "Best Accuracy: 0.43\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import optuna\n",
    "import numpy as np\n",
    "\n",
    "# 定义数据生成器\n",
    "def generate_data(N):\n",
    "    data = np.random.rand(N, 1, 32, 16).astype(np.float32)\n",
    "    labels = np.random.randint(2, size=N)\n",
    "    return torch.tensor(data), torch.tensor(labels)\n",
    "\n",
    "# 定义CNN模型类\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self, num_classes, conv1_kernel_size, conv2_kernel_size):\n",
    "        super(CNNModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=conv1_kernel_size, padding=(conv1_kernel_size-1)//2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=conv2_kernel_size, padding=(conv2_kernel_size-1)//2)\n",
    "        self.fc1 = nn.Linear(64 * 8 * 4, 128)  # 根据数据大小进行调整\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.max_pool2d(x, 2)\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = torch.max_pool2d(x, 2)\n",
    "        x = x.view(-1, 64 * 8 * 4)  # 根据数据大小进行调整\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# 定义训练函数\n",
    "def train(model, dataloader, criterion, optimizer, num_epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        for inputs, labels in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "# 定义评估函数\n",
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    return correct / total\n",
    "\n",
    "# 定义Optuna试验函数\n",
    "def objective(trial):\n",
    "    # 定义超参数搜索范围\n",
    "    conv1_kernel_size = trial.suggest_int(\"conv1_kernel_size\", 3, 5, step=2)\n",
    "    conv2_kernel_size = trial.suggest_int(\"conv2_kernel_size\", 1, 3, step=2)\n",
    "    \n",
    "    # 创建数据加载器\n",
    "    train_data, train_labels = generate_data(1000)\n",
    "    train_dataset = torch.utils.data.TensorDataset(train_data, train_labels)\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    \n",
    "    val_data, val_labels = generate_data(200)\n",
    "    val_dataset = torch.utils.data.TensorDataset(val_data, val_labels)\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "    \n",
    "    # 创建模型\n",
    "    model = CNNModel(num_classes=2, conv1_kernel_size=conv1_kernel_size, conv2_kernel_size=conv2_kernel_size)\n",
    "    \n",
    "    # 定义损失函数和优化器\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    # 训练模型\n",
    "    train(model, train_loader, criterion, optimizer, num_epochs=5)\n",
    "    \n",
    "    # 评估模型\n",
    "    accuracy = evaluate(model, val_loader)\n",
    "    \n",
    "    return -accuracy  # 我们希望最大化召回率，因此返回负的准确度\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(objective, n_trials=10)  # 运行10次试验\n",
    "    best_params = study.best_params\n",
    "    best_accuracy = -study.best_value\n",
    "    print(f\"Best Parameters: {best_params}\")\n",
    "    print(f\"Best Accuracy: {best_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1a1a28-433b-4771-a9dc-2bc94978831c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## transfomer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7c7361a6-790d-46f2-8134-9868da8f8153",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前设备: cuda\n",
      "Epoch [1/1], Loss: 0.6972571015357971\n",
      "(32, 1, 32, 16)\n",
      "(32, 1, 32, 16)\n",
      "(32, 1, 32, 16)\n",
      "(32, 1, 32, 16)\n",
      "(32, 1, 32, 16)\n",
      "(32, 1, 32, 16)\n",
      "(8, 1, 32, 16)\n",
      "Test Accuracy: 47.0%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "\n",
    "# 准备数据\n",
    "N = 1000  # 假设有1000个样本\n",
    "image_size = (1, 32, 16)\n",
    "num_classes = 2\n",
    "data = torch.randn(N, *image_size)\n",
    "labels = torch.randint(0, num_classes, (N,))\n",
    "\n",
    "# 划分训练集和测试集\n",
    "split_ratio = 0.8\n",
    "split_idx = int(N * split_ratio)\n",
    "train_data, test_data = data[:split_idx], data[split_idx:]\n",
    "train_labels, test_labels = labels[:split_idx], labels[split_idx:]\n",
    "\n",
    "# 创建数据加载器\n",
    "batch_size = 32\n",
    "train_dataset = TensorDataset(train_data, train_labels)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataset = TensorDataset(test_data, test_labels)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# 创建ViT模型\n",
    "class ViT(nn.Module):\n",
    "    def __init__(self, image_size, num_classes, patch_size, hidden_dim, num_heads, num_layers):\n",
    "        super(ViT, self).__init__()\n",
    "        self.embedding = nn.Conv2d(image_size[0], hidden_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=hidden_dim,\n",
    "            nhead=num_heads,\n",
    "            num_encoder_layers=num_layers,\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = x.view(x.size(0), x.size(1), -1)  # Flatten spatial dimensions\n",
    "        x = x.permute(2, 0, 1)  # Adjust the dimension order\n",
    "        x = self.transformer(x, x)  # Self-attention with source and target being the same\n",
    "        x = x.mean(dim=0)  # Global average pooling\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# 创建ViT模型实例\n",
    "model = ViT(image_size, num_classes, patch_size=(8, 8), hidden_dim=64, num_heads=4, num_layers=2)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"当前设备:\", device)\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 训练模型\n",
    "num_epochs = 1\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for data, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        data_gpu = data.to(device)\n",
    "        labels_gpu = labels.to(device)\n",
    "        outputs = model(data_gpu)\n",
    "        loss = criterion(outputs, labels_gpu)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item()}')\n",
    "\n",
    "# 在测试集上评估模型\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, labels in test_loader:\n",
    "        data_gpu = data.to(device)\n",
    "        labels_gpu = labels.to(device)\n",
    "        outputs = model(data_gpu)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels_gpu).sum().item()\n",
    "        \n",
    "        curdata = np.array(data)\n",
    "        print(curdata.shape)\n",
    "        \n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f'Test Accuracy: {accuracy}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "01ea800d-db34-47cc-a34c-5844bdf75166",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "满足条件的行数：3\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 创建一个示例的二维NumPy矩阵\n",
    "matrix = np.array([[1, 2, 3],\n",
    "                  [-1, 0, 2],\n",
    "                  [0, 0, 0],\n",
    "                  [4, 5, 6]])\n",
    "\n",
    "# 计算每行的和\n",
    "row_sums = np.sum(matrix, axis=1)\n",
    "\n",
    "# 使用向量化操作检查每行的和是否大于0，得到一个布尔值的矩阵\n",
    "row_sum_greater_than_zero = row_sums > 0\n",
    "\n",
    "# 统计满足条件的行数\n",
    "count = np.sum(row_sum_greater_than_zero)\n",
    "\n",
    "print(f\"满足条件的 行数：{count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "891ae08c-0b64-44af-b4d6-3d5489167978",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES']='1'\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cb55fe04-9d73-486c-ad68-beb6d2b1d50e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03770c7c-b824-494f-b944-6ffb32c84192",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前设备: cuda:1\n",
      "Epoch [1/1], Loss: 0.7072823643684387\n",
      "Test Accuracy: 44.0%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "\n",
    "# 准备数据\n",
    "N = 1000  # 假设有1000个样本\n",
    "image_size = (1, 32, 16)\n",
    "num_classes = 2\n",
    "data = torch.randn(N, *image_size)\n",
    "labels = torch.randint(0, num_classes, (N,))\n",
    "\n",
    "# 划分训练集和测试集\n",
    "split_ratio = 0.8\n",
    "split_idx = int(N * split_ratio)\n",
    "train_data, test_data = data[:split_idx], data[split_idx:]\n",
    "train_labels, test_labels = labels[:split_idx], labels[split_idx:]\n",
    "\n",
    "# 创建数据加载器\n",
    "batch_size = 32\n",
    "train_dataset = TensorDataset(train_data, train_labels)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataset = TensorDataset(test_data, test_labels)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# 创建ViT模型\n",
    "class ViT(nn.Module):\n",
    "    def __init__(self, image_size, num_classes, patch_size, hidden_dim, num_heads, num_layers):\n",
    "        super(ViT, self).__init__()\n",
    "        self.embedding = nn.Conv2d(image_size[0], hidden_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=hidden_dim,\n",
    "            nhead=num_heads,\n",
    "            num_encoder_layers=num_layers,\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = x.view(x.size(0), x.size(1), -1)  # Flatten spatial dimensions\n",
    "        x = x.permute(2, 0, 1)  # Adjust the dimension order\n",
    "        x = self.transformer(x, x)  # Self-attention with source and target being the same\n",
    "        x = x.mean(dim=0)  # Global average pooling\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# 创建ViT模型实例\n",
    "model = ViT(image_size, num_classes, patch_size=(8, 8), hidden_dim=64, num_heads=4, num_layers=4)\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"当前设备:\", device)\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 训练模型\n",
    "num_epochs = 1\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for data, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        data_gpu = data.to(device)\n",
    "        labels_gpu = labels.to(device)\n",
    "        outputs = model(data_gpu)\n",
    "        loss = criterion(outputs, labels_gpu)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # print(f\"{loss}\")\n",
    "\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item()}')\n",
    "\n",
    "# 在测试集上评估模型\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, labels in test_loader:\n",
    "        data_gpu = data.to(device)\n",
    "        labels_gpu = labels.to(device)\n",
    "        outputs = model(data_gpu)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels_gpu).sum().item()\n",
    "        \n",
    "        curdata = np.array(data)\n",
    "        # print(curdata.shape)\n",
    "        \n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f'Test Accuracy: {accuracy}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "708e4f47-8287-4eb0-b383-3e38784a6306",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "非零元素的最大行索引：3\n",
      "非零元素的最小行索引：1\n",
      "行数之间的行数：3\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 创建一个示例的二维NumPy矩阵\n",
    "matrix = np.array([[0, 0, 0],\n",
    "                  [1, 2, 3],\n",
    "                  [0, 0, 0],\n",
    "                  [4, 5, 6],\n",
    "                  [0, 0, 0]])\n",
    "\n",
    "# 找到非零元素的行索引\n",
    "non_zero_rows = np.any(matrix != 0, axis=1)\n",
    "\n",
    "# 找到非零元素的最大和最小行索引\n",
    "max_non_zero_row_index = np.max(np.where(non_zero_rows))\n",
    "min_non_zero_row_index = np.min(np.where(non_zero_rows))\n",
    "\n",
    "# 计算最大和最小行索引之间的行数\n",
    "row_count_between = max_non_zero_row_index - min_non_zero_row_index + 1\n",
    "\n",
    "print(f\"非零元素的最大行索引：{max_non_zero_row_index}\")\n",
    "print(f\"非零元素的最小行索引：{min_non_zero_row_index}\")\n",
    "print(f\"行数之间的行数：{row_count_between}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6ff70b9-0892-4a00-86ef-40f2d4542c84",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "非零元素的最大行索引：3\n",
      "非零元素的最小列索引：0\n",
      "列数之间的列数：4\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 创建一个示例的二维NumPy矩阵\n",
    "matrix = np.array([[0, 0, 0, 1, 0],\n",
    "                  [0, 0, 2, 0, 0],\n",
    "                  [0, 0, 0, 0, 0],\n",
    "                  [4, 5, 0, 0, 0],\n",
    "                  [0, 0, 0, 0, 0]])\n",
    "\n",
    "# 找到非零元素的行索引和列索引\n",
    "non_zero_rows, non_zero_cols = np.nonzero(matrix)\n",
    "\n",
    "# 找到非零元素的最大行索引和最小列索引\n",
    "max_non_zero_row_index = np.max(non_zero_rows)\n",
    "min_non_zero_col_index = np.min(non_zero_cols)\n",
    "\n",
    "# 计算最小列索引与最大列索引之间的列数\n",
    "column_count_between = max_non_zero_row_index - min_non_zero_col_index + 1\n",
    "\n",
    "print(f\"非零元素的最大行索引：{max_non_zero_row_index}\")\n",
    "print(f\"非零元素的最小列索引：{min_non_zero_col_index}\")\n",
    "print(f\"列数之间的列数：{column_count_between}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "445d2452-2bc0-43a9-b3a3-a3a48e36baf7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "matrix = np.array([2,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee09a028-555d-4a92-bde6-0b594166347a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38bbfa46-f0d6-4023-bc15-43ebb945f513",
   "metadata": {},
   "source": [
    "## cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5a0f714b-edcc-48fd-b207-124a8cb47dd9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-22 15:48:08,297 - INFO - Epoch 1/100, Loss: 0.6953883015426101\n",
      "2023-09-22 15:48:08,990 - INFO - Epoch 2/100, Loss: 0.6916598795325892\n",
      "2023-09-22 15:48:09,691 - INFO - Epoch 3/100, Loss: 0.6874425490950323\n",
      "2023-09-22 15:48:10,379 - INFO - Epoch 4/100, Loss: 0.682596045314886\n",
      "2023-09-22 15:48:11,076 - INFO - Epoch 5/100, Loss: 0.6756010940120478\n",
      "2023-09-22 15:48:11,770 - INFO - Epoch 6/100, Loss: 0.663198435762126\n",
      "2023-09-22 15:48:12,477 - INFO - Epoch 7/100, Loss: 0.6513456436479168\n",
      "2023-09-22 15:48:13,184 - INFO - Epoch 8/100, Loss: 0.6371911600896507\n",
      "2023-09-22 15:48:13,874 - INFO - Epoch 9/100, Loss: 0.6248618831300432\n",
      "2023-09-22 15:48:14,574 - INFO - Epoch 10/100, Loss: 0.6107565339203853\n",
      "2023-09-22 15:48:15,289 - INFO - Epoch 11/100, Loss: 0.5968862639111319\n",
      "2023-09-22 15:48:15,995 - INFO - Epoch 12/100, Loss: 0.5774357652967903\n",
      "2023-09-22 15:48:16,761 - INFO - Epoch 13/100, Loss: 0.5662180661775504\n",
      "2023-09-22 15:48:17,523 - INFO - Epoch 14/100, Loss: 0.5551156170049291\n",
      "2023-09-22 15:48:18,263 - INFO - Epoch 15/100, Loss: 0.5385837554931641\n",
      "2023-09-22 15:48:18,979 - INFO - Epoch 16/100, Loss: 0.5266512395090358\n",
      "2023-09-22 15:48:19,691 - INFO - Epoch 17/100, Loss: 0.5161721275490561\n",
      "2023-09-22 15:48:20,400 - INFO - Epoch 18/100, Loss: 0.5058401705353124\n",
      "2023-09-22 15:48:21,083 - INFO - Epoch 19/100, Loss: 0.49101464023256\n",
      "2023-09-22 15:48:21,781 - INFO - Epoch 20/100, Loss: 0.4852368983493489\n",
      "2023-09-22 15:48:22,479 - INFO - Epoch 21/100, Loss: 0.4811803947208793\n",
      "2023-09-22 15:48:23,159 - INFO - Epoch 22/100, Loss: 0.46462134608797206\n",
      "2023-09-22 15:48:23,870 - INFO - Epoch 23/100, Loss: 0.46134654370842465\n",
      "2023-09-22 15:48:24,590 - INFO - Epoch 24/100, Loss: 0.4478969408827982\n",
      "2023-09-22 15:48:25,316 - INFO - Epoch 25/100, Loss: 0.4384247778327602\n",
      "2023-09-22 15:48:26,022 - INFO - Epoch 26/100, Loss: 0.4309878501163167\n",
      "2023-09-22 15:48:26,741 - INFO - Epoch 27/100, Loss: 0.42678605504096695\n",
      "2023-09-22 15:48:27,434 - INFO - Epoch 28/100, Loss: 0.4188296711368925\n",
      "2023-09-22 15:48:28,130 - INFO - Epoch 29/100, Loss: 0.4102428236584755\n",
      "2023-09-22 15:48:28,836 - INFO - Epoch 30/100, Loss: 0.40009143834660765\n",
      "2023-09-22 15:48:29,533 - INFO - Epoch 31/100, Loss: 0.4003088886191131\n",
      "2023-09-22 15:48:30,226 - INFO - Epoch 32/100, Loss: 0.38924826169089904\n",
      "2023-09-22 15:48:30,919 - INFO - Epoch 33/100, Loss: 0.3864704217690571\n",
      "2023-09-22 15:48:31,610 - INFO - Epoch 34/100, Loss: 0.38089401763715564\n",
      "2023-09-22 15:48:32,300 - INFO - Epoch 35/100, Loss: 0.3735602929903443\n",
      "2023-09-22 15:48:32,995 - INFO - Epoch 36/100, Loss: 0.3677509932950803\n",
      "2023-09-22 15:48:33,686 - INFO - Epoch 37/100, Loss: 0.35855129702835326\n",
      "2023-09-22 15:48:34,380 - INFO - Epoch 38/100, Loss: 0.35852713284978444\n",
      "2023-09-22 15:48:35,071 - INFO - Epoch 39/100, Loss: 0.3467830485029585\n",
      "2023-09-22 15:48:35,766 - INFO - Epoch 40/100, Loss: 0.34432405745907196\n",
      "2023-09-22 15:48:36,452 - INFO - Epoch 41/100, Loss: 0.33487671347940046\n",
      "2023-09-22 15:48:37,156 - INFO - Epoch 42/100, Loss: 0.33234301949762235\n",
      "2023-09-22 15:48:37,859 - INFO - Epoch 43/100, Loss: 0.3237035807910239\n",
      "2023-09-22 15:48:38,560 - INFO - Epoch 44/100, Loss: 0.32071583542474513\n",
      "2023-09-22 15:48:39,250 - INFO - Epoch 45/100, Loss: 0.31427600286948454\n",
      "2023-09-22 15:48:39,940 - INFO - Epoch 46/100, Loss: 0.3128359609158935\n",
      "2023-09-22 15:48:40,621 - INFO - Epoch 47/100, Loss: 0.3069607961899156\n",
      "2023-09-22 15:48:41,305 - INFO - Epoch 48/100, Loss: 0.3009675958543826\n",
      "2023-09-22 15:48:42,008 - INFO - Epoch 49/100, Loss: 0.2975669972076537\n",
      "2023-09-22 15:48:42,712 - INFO - Epoch 50/100, Loss: 0.2970070267558857\n",
      "2023-09-22 15:48:43,413 - INFO - Epoch 51/100, Loss: 0.28718046711128986\n",
      "2023-09-22 15:48:44,121 - INFO - Epoch 52/100, Loss: 0.2853987874688616\n",
      "2023-09-22 15:48:44,821 - INFO - Epoch 53/100, Loss: 0.2814540098996679\n",
      "2023-09-22 15:48:45,521 - INFO - Epoch 54/100, Loss: 0.2770563442331211\n",
      "2023-09-22 15:48:46,227 - INFO - Epoch 55/100, Loss: 0.27510267657459164\n",
      "2023-09-22 15:48:46,920 - INFO - Epoch 56/100, Loss: 0.2689265891150304\n",
      "2023-09-22 15:48:47,597 - INFO - Epoch 57/100, Loss: 0.2689273706666983\n",
      "2023-09-22 15:48:48,284 - INFO - Epoch 58/100, Loss: 0.261792340570954\n",
      "2023-09-22 15:48:48,980 - INFO - Epoch 59/100, Loss: 0.2605343996339543\n",
      "2023-09-22 15:48:49,667 - INFO - Epoch 60/100, Loss: 0.2527102581254996\n",
      "2023-09-22 15:48:50,355 - INFO - Epoch 61/100, Loss: 0.2517929109418468\n",
      "2023-09-22 15:48:51,051 - INFO - Epoch 62/100, Loss: 0.24889111988673543\n",
      "2023-09-22 15:48:51,734 - INFO - Epoch 63/100, Loss: 0.24506680259279384\n",
      "2023-09-22 15:48:52,427 - INFO - Epoch 64/100, Loss: 0.23426931479554267\n",
      "2023-09-22 15:48:53,119 - INFO - Epoch 65/100, Loss: 0.23228364452055306\n",
      "2023-09-22 15:48:53,820 - INFO - Epoch 66/100, Loss: 0.22801755629717166\n",
      "2023-09-22 15:48:54,510 - INFO - Epoch 67/100, Loss: 0.22978351170280178\n",
      "2023-09-22 15:48:55,221 - INFO - Epoch 68/100, Loss: 0.22988679919652877\n",
      "2023-09-22 15:48:55,910 - INFO - Epoch 69/100, Loss: 0.2209054903145049\n",
      "2023-09-22 15:48:56,615 - INFO - Epoch 70/100, Loss: 0.2137931817845934\n",
      "2023-09-22 15:48:57,330 - INFO - Epoch 71/100, Loss: 0.2134029529751486\n",
      "2023-09-22 15:48:58,054 - INFO - Epoch 72/100, Loss: 0.21176558884845417\n",
      "2023-09-22 15:48:58,786 - INFO - Epoch 73/100, Loss: 0.21126431065380194\n",
      "2023-09-22 15:48:59,507 - INFO - Epoch 74/100, Loss: 0.20278464922100115\n",
      "2023-09-22 15:49:00,203 - INFO - Epoch 75/100, Loss: 0.1966707101388342\n",
      "2023-09-22 15:49:00,898 - INFO - Epoch 76/100, Loss: 0.1968153321724029\n",
      "2023-09-22 15:49:01,595 - INFO - Epoch 77/100, Loss: 0.18888871153448797\n",
      "2023-09-22 15:49:02,292 - INFO - Epoch 78/100, Loss: 0.19061259246745688\n",
      "2023-09-22 15:49:02,981 - INFO - Epoch 79/100, Loss: 0.19459337423181838\n",
      "2023-09-22 15:49:03,678 - INFO - Epoch 80/100, Loss: 0.1821250825835641\n",
      "2023-09-22 15:49:04,364 - INFO - Epoch 81/100, Loss: 0.17983450854470015\n",
      "2023-09-22 15:49:05,067 - INFO - Epoch 82/100, Loss: 0.17519370423760383\n",
      "2023-09-22 15:49:05,841 - INFO - Epoch 83/100, Loss: 0.17483966826063813\n",
      "2023-09-22 15:49:06,610 - INFO - Epoch 84/100, Loss: 0.1736368473358215\n",
      "2023-09-22 15:49:07,371 - INFO - Epoch 85/100, Loss: 0.17165724335202745\n",
      "2023-09-22 15:49:08,151 - INFO - Epoch 86/100, Loss: 0.16574523422368773\n",
      "2023-09-22 15:49:08,948 - INFO - Epoch 87/100, Loss: 0.16902650332754585\n",
      "2023-09-22 15:49:09,707 - INFO - Epoch 88/100, Loss: 0.16149368891670446\n",
      "2023-09-22 15:49:10,457 - INFO - Epoch 89/100, Loss: 0.15412393772298363\n",
      "2023-09-22 15:49:11,204 - INFO - Epoch 90/100, Loss: 0.1568150494223947\n",
      "2023-09-22 15:49:11,933 - INFO - Epoch 91/100, Loss: 0.1499471706901766\n",
      "2023-09-22 15:49:12,628 - INFO - Epoch 92/100, Loss: 0.15291747749800894\n",
      "2023-09-22 15:49:13,331 - INFO - Epoch 93/100, Loss: 0.1501680184511622\n",
      "2023-09-22 15:49:14,026 - INFO - Epoch 94/100, Loss: 0.147336924674025\n",
      "2023-09-22 15:49:14,719 - INFO - Epoch 95/100, Loss: 0.14150860426342413\n",
      "2023-09-22 15:49:15,427 - INFO - Epoch 96/100, Loss: 0.144796487726983\n",
      "2023-09-22 15:49:16,138 - INFO - Epoch 97/100, Loss: 0.1339317005436132\n",
      "2023-09-22 15:49:16,835 - INFO - Epoch 98/100, Loss: 0.14775098736878414\n",
      "2023-09-22 15:49:17,519 - INFO - Epoch 99/100, Loss: 0.1348300286254306\n",
      "2023-09-22 15:49:18,199 - INFO - Epoch 100/100, Loss: 0.13178297872566114\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DataParallel' object has no attribute 'conv1'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 83\u001b[0m\n\u001b[1;32m     80\u001b[0m correct \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     81\u001b[0m total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 83\u001b[0m target_layer \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m  \u001b[38;5;66;03m# Assuming 'conv2' is the layer you want to visualize\u001b[39;00m\n\u001b[1;32m     84\u001b[0m gradcam \u001b[38;5;241m=\u001b[39m GradCAM(model, target_layer)\n\u001b[1;32m     85\u001b[0m \u001b[38;5;66;03m# gradcam = GradCAM(model, target_layer='conv2')  # Replace 'conv2' with the layer you want to visualize\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/dram-py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1614\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1612\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1613\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1614\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1615\u001b[0m     \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, name))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataParallel' object has no attribute 'conv1'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from gradcam import GradCAM\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "import logging\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "\n",
    "N=10000\n",
    "M=100\n",
    "# Define the CNN model\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, 3, padding=1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "        self.fc = nn.Linear(32 * 8 * 4, 2)  # 32 * 8 * 4 is the output size after 2 conv layers\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(self.relu1(self.conv1(x)))\n",
    "        x = self.pool2(self.relu2(self.conv2(x)))\n",
    "        x = x.view(-1, 32 * 8 * 4)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Load and preprocess your data\n",
    "# Assuming you have your data as tensors X_train, y_train, X_test, and y_test\n",
    "# Convert them into PyTorch tensors and datasets\n",
    "\n",
    "X_train = torch.randn(N, 1, 32, 16)\n",
    "y_train = torch.randint(2, (N,))\n",
    "X_test = torch.randn(M, 1, 32, 16)\n",
    "y_test = torch.randint(2, (M,))\n",
    "\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = SimpleCNN()\n",
    "model.to(\"cuda\")\n",
    "\n",
    "model = nn.DataParallel(model)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputss, labelss = inputs.to(\"cuda\"),labels.to(\"cuda\") \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputss)\n",
    "        loss = criterion(outputs, labelss)\n",
    "        loss.backward()\n",
    "        running_loss += loss.item()\n",
    "        optimizer.step()\n",
    "    logging.info(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {running_loss / len(train_loader)}\")\n",
    "\n",
    "# Test the model\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "target_layer = model.conv1  # Assuming 'conv2' is the layer you want to visualize\n",
    "gradcam = GradCAM(model, target_layer)\n",
    "# gradcam = GradCAM(model, target_layer='conv2')  # Replace 'conv2' with the layer you want to visualize\n",
    "\n",
    "for inputs, labels in test_loader:\n",
    "    inputss, labelss = inputs.to(\"cuda\"),labels.to(\"cuda\") \n",
    "    outputs = model(inputss)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labelss).sum().item()\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = 100 * correct / total\n",
    "print(f'Test Accuracy: {accuracy:.2f}%')\n",
    "\n",
    "model.to(\"cpu\")\n",
    "positive_heatmaps = []\n",
    "negative_heatmaps = []\n",
    "for inputs, labels in test_loader:\n",
    "    # inputss, labelss = inputs.to(\"cuda\"),labels.to(\"cuda\") \n",
    "    outputs = model(inputs)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "    for i in range(inputs.size(0)):\n",
    "        if predicted[i] == labels[i]:\n",
    "            # Correctly classified samples\n",
    "            heatmap, _ = gradcam(inputs[i].unsqueeze(0))\n",
    "            heatmap = heatmap.squeeze().numpy()  # Extract the heatmap and convert to NumPy array\n",
    "            if predicted[i] == 1:\n",
    "                positive_heatmaps.append(heatmap)\n",
    "            else:\n",
    "                negative_heatmaps.append(heatmap)\n",
    "average_positive_heatmap = np.mean(positive_heatmaps, axis=0)\n",
    "average_negative_heatmap = np.mean(negative_heatmaps, axis=0)\n",
    "heatmap_difference = average_positive_heatmap - average_negative_heatmap\n",
    "threshold = 0.1  # 设定差异阈值，根据实际情况调整\n",
    "heatmap_difference[heatmap_difference < threshold] = 0\n",
    "\n",
    "print(average_positive_heatmap)\n",
    "\n",
    "average_negative_heatmap = average_negative_heatmap / 10000\n",
    "print(average_negative_heatmap)\n",
    "\n",
    "# 可视化差异区域\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(average_positive_heatmap, cmap='jet')\n",
    "plt.title('Average Grad-CAM for Correct Positive Samples')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(average_negative_heatmap, cmap='jet')\n",
    "plt.title('Average Grad-CAM for Correct Negative Samples')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(heatmap_difference, cmap='jet')\n",
    "plt.title('Heatmap Difference')\n",
    "plt.axis('off')\n",
    "        #     # Plot the heatmap\n",
    "        #     plt.figure(figsize=(6, 6))\n",
    "        #     plt.imshow(heatmap, cmap='jet')\n",
    "        #     plt.title(f'Correct: Predicted={predicted[i]}, Actual={labels[i]}')\n",
    "        #     plt.axis('off')\n",
    "        #     plt.show()\n",
    "        # else:\n",
    "        #     # Incorrectly classified samples (if needed)\n",
    "        #     pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "562d468f-173f-475b-a3f7-44947dd65829",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (16x384 and 576x128)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 56\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inputs, labels \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m     55\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 56\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze()  \u001b[38;5;66;03m# Squeeze the output to match the labels' shape\u001b[39;00m\n\u001b[1;32m     57\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels\u001b[38;5;241m.\u001b[39mfloat())  \u001b[38;5;66;03m# Use BCEWithLogitsLoss for binary classification\u001b[39;00m\n\u001b[1;32m     58\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/anaconda3/envs/dram-py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[34], line 23\u001b[0m, in \u001b[0;36mCNN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     21\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool(torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x)))\n\u001b[1;32m     22\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Flatten the tensor\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     24\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(x)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/anaconda3/envs/dram-py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/dram-py310/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (16x384 and 576x128)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Define the CNN model\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3)\n",
    "        self.fc1 = nn.Linear(32 * 6 * 3, 128)\n",
    "        self.fc2 = nn.Linear(128, 1)  # 1 output for binary classification\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x)))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = x.view(x.size(0), -1)  # Flatten the tensor\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Generate sample data and labels (replace with your own data)\n",
    "N = 100  # Replace with your actual data size\n",
    "data = np.random.randn(N, 1, 32, 16)\n",
    "labels = np.random.randint(0, 2, N)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "split_ratio = 0.8\n",
    "split_idx = int(N * split_ratio)\n",
    "train_data, test_data = data[:split_idx], data[split_idx:]\n",
    "train_labels, test_labels = labels[:split_idx], labels[split_idx:]\n",
    "\n",
    "# Create a DataLoader (you might need to adjust batch size and other parameters)\n",
    "batch_size = 16\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    list(zip(train_data, train_labels)),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "# Initialize the CNN model, loss function, and optimizer\n",
    "model = CNN()\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop (adjust as needed)\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs.float()).squeeze()  # Squeeze the output to match the labels' shape\n",
    "        loss = criterion(outputs, labels.float())  # Use BCEWithLogitsLoss for binary classification\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Evaluation on the test set\n",
    "test_data = torch.tensor(test_data).float()\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    outputs = model(test_data).squeeze()  # Squeeze the output to match the labels' shape\n",
    "    predicted_labels = (torch.sigmoid(outputs) > 0.5).float()  # Convert logits to binary predictions\n",
    "\n",
    "# Extract feature maps for correctly classified samples\n",
    "correct_samples = test_data[(predicted_labels == test_labels).bool()]\n",
    "correct_labels = test_labels[(predicted_labels == test_labels).bool()]\n",
    "\n",
    "# Define a hook to extract feature maps from the second convolutional layer\n",
    "feature_maps = []\n",
    "\n",
    "def hook_fn(module, input, output):\n",
    "    feature_maps.append(output)\n",
    "\n",
    "hook = model.conv2.register_forward_hook(hook_fn)\n",
    "\n",
    "# Forward pass for correctly classified samples to extract feature maps\n",
    "_ = model(correct_samples)\n",
    "\n",
    "# Remove the hook\n",
    "hook.remove()\n",
    "\n",
    "# Apply t-SNE to the feature maps\n",
    "feature_maps = torch.cat(feature_maps, dim=0)\n",
    "feature_maps = feature_maps.view(feature_maps.size(0), -1)  # Flatten feature maps\n",
    "\n",
    "tsne = TSNE(n_components=2)\n",
    "tsne_result = tsne.fit_transform(feature_maps.numpy())  # Convert to NumPy array for t-SNE\n",
    "\n",
    "# Separate positive and negative samples based on labels\n",
    "positive_indices = (correct_labels == 1).bool()\n",
    "negative_indices = (correct_labels == 0).bool()\n",
    "\n",
    "# Plot t-SNE results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(tsne_result[positive_indices, 0], tsne_result[positive_indices, 1], c='g', label='Positive')\n",
    "plt.scatter(tsne_result[negative_indices, 0], tsne_result[negative_indices, 1], c='r', label='Negative')\n",
    "plt.title(\"t-SNE Visualization of Correctly Classified Sample Feature Maps\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "19890c1e-ce89-4c5e-978c-f549c00e34ed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TwoLayerCNN(\n",
       "  (conv1): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (relu1): ReLU()\n",
       "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (relu2): ReLU()\n",
       "  (fc): Linear(in_features=32, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class TwoLayerCNN(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels1, out_channels2, kernel_size1, kernel_size2):\n",
    "        super(TwoLayerCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels1, kernel_size=kernel_size1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(out_channels1, out_channels2, kernel_size=kernel_size2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc = nn.Linear(out_channels2, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten the tensor\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# 指定模型参数\n",
    "in_channels = 1  # 输入通道数\n",
    "out_channels1 = 16  # 第一层卷积核数量\n",
    "out_channels2 = 32  # 第二层卷积核数量\n",
    "kernel_size1 = 3  # 第一层卷积核大小\n",
    "kernel_size2 = 3  # 第二层卷积核大小\n",
    "num_classes = 10  # 分类类别数量\n",
    "\n",
    "# 创建模型实例\n",
    "model = TwoLayerCNN(in_channels, out_channels1, out_channels2, kernel_size1, kernel_size2)\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 训练模型（假设有训练数据和标签，以下是伪代码）\n",
    "# for epoch in range(num_epochs):\n",
    "#     for batch in train_loader:\n",
    "#         data, labels = batch\n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = model(data)\n",
    "#         loss = criterion(outputs, labels)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "# 保存模型参数到文件\n",
    "torch.save(model.state_dict(), 'two_layer_cnn_model.pth')\n",
    "\n",
    "# 加载模型\n",
    "loaded_model = TwoLayerCNN(in_channels, out_channels1, out_channels2, kernel_size1, kernel_size2)\n",
    "loaded_model.load_state_dict(torch.load('two_layer_cnn_model.pth'))\n",
    "loaded_model.eval()  # 进入评估模式，用于推断\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fcddf7a2-4049-4d10-84cc-d1ba473d5353",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server_22848\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# 目标字符串\n",
    "target_string = \"../data/processed/agg/Server_22848_1_feats_32x16_11.npy\"\n",
    "\n",
    "# 定义正则表达式模式，匹配\"Server_\"后面跟着数字的部分\n",
    "pattern = r\"Server_(\\d+)\"\n",
    "\n",
    "# 使用正则表达式查找匹配的内容\n",
    "match = re.search(pattern, target_string)\n",
    "\n",
    "# 提取匹配的部分\n",
    "if match:\n",
    "    extracted_string = match.group(0)\n",
    "\n",
    "    # 输出提取的字符串\n",
    "    print(extracted_string)\n",
    "else:\n",
    "    print(\"未找到匹配的字符串\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7989f32-4c03-4bd3-ac2f-fffcb6f35cfc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dram-py310",
   "language": "python",
   "name": "dram-py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
