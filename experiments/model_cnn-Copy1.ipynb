{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d48769c5-a167-4706-9639-50aa64fe0e6f",
   "metadata": {},
   "source": [
    "# header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4279f410-2517-4234-ade1-768976367c03",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import datetime\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from numba import cuda\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "import config as CONFIG\n",
    "import utils\n",
    "\n",
    "import wandb\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset, SubsetRandomSampler\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f916e65-2402-4c07-8747-1a6ba7a2920d",
   "metadata": {},
   "source": [
    "# generate features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7a58be63-e1e5-40ac-a692-2f34d8007475",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# single process, go \"feats.py\" for multi process\n",
    "max_row = 131072\n",
    "max_col = 1024\n",
    "row_block = 32\n",
    "col_block = 16\n",
    "row_section_size = max_row // row_block\n",
    "col_section_size = max_col // col_block\n",
    "\n",
    "def process_one(i):\n",
    "    df = pd.read_csv(os.path.join(CONFIG.PATH_PROCESSED, f\"mcelog_{i}.csv\"))\n",
    "    data = []\n",
    "    labels = []\n",
    "    hosts = df[\"sid\"].drop_duplicates().to_list()\n",
    "\n",
    "    for host in tqdm(hosts):\n",
    "        host_matrix = np.zeros([row_block, col_block])\n",
    "        host_df = df[df[\"sid\"]==host]\n",
    "        host_df = host_df.fillna(-1)\n",
    "        \n",
    "        label = (int(host_df[\"failure_type\"].sum() < 0) + 1) % 2\n",
    "        labels.append(label)\n",
    "\n",
    "        host_df[\"row_section\"] = host_df[\"row\"] // row_section_size\n",
    "        host_df[\"col_section\"] = host_df[\"col\"] // col_section_size\n",
    "\n",
    "        host_df_simple = host_df[[\"row_section\", \"col_section\"]].drop_duplicates()\n",
    "\n",
    "        for _, row in host_df_simple.iterrows():\n",
    "            host_matrix[int(row[\"row_section\"]), int(row[\"col_section\"])] = 1\n",
    "        data.append(host_matrix)\n",
    "    \n",
    "    np.save(os.path.join(CONFIG.PATH_PROCESSED, f\"feats_{i}.npy\"), data)\n",
    "    np.save(os.path.join(CONFIG.PATH_PROCESSED, f\"labels_{i}.npy\"), labels)\n",
    "    \n",
    "    \n",
    "# for i in range(0, 31):\n",
    "#     if i == 22:\n",
    "#         continue\n",
    "#     process_one(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47a0222-9191-4d9c-8f15-a1817ced8570",
   "metadata": {},
   "source": [
    "# load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8f7f747c-4472-4aa2-ade0-a1405eb3f027",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0. , 0. , 0. ],\n",
       "       [0. , 2.2, 0. ],\n",
       "       [0. , 0. , 0. ]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "host_matrix = np.zeros([3, 3])\n",
    "\n",
    "host_matrix[1,1] += 1\n",
    "host_matrix[1,1] += 1.2\n",
    "\n",
    "host_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e9635899-78b8-4ee0-b238-59fdfd738ca1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "dataset_list = []\n",
    "\n",
    "\n",
    "for i in range(31):\n",
    "    if i == 22:\n",
    "        continue\n",
    "    data_np = np.load(os.path.join(CONFIG.PATH_PROCESSED, f\"feats_64x32_with_times_{i}.npy\"))\n",
    "    label_np = np.load(os.path.join(CONFIG.PATH_PROCESSED, f\"labels_64x32_{i}.npy\"))\n",
    "    \n",
    "    data_np = 2 / (1 + np.exp(-data_np)) - 1\n",
    "\n",
    "    data = torch.tensor(data_np, dtype=torch.float32)\n",
    "    label = torch.tensor(label_np, dtype=torch.int64)\n",
    "     \n",
    "    dataset = TensorDataset(data, label)\n",
    "    dataset_list.append(dataset)\n",
    "\n",
    "datasets = torch.utils.data.ConcatDataset(dataset_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "88336636-7047-4564-9336-d1378f9b1221",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_np[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc73cd8-13e5-43f3-879d-d8f0804f459b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## check ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1429df70-0e52-4d73-9e94-7846a59196ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "dataset_list = []\n",
    "\n",
    "xgb_data_list = []\n",
    "xgb_label_list = []\n",
    "\n",
    "cnt = 0\n",
    "error = 0\n",
    "enrich_ratio = 1\n",
    "under_sampling_ratio = 8\n",
    "\n",
    "for i in range(31):\n",
    "    if i == 22:\n",
    "        continue\n",
    "    data_np = np.load(os.path.join(CONFIG.PATH_PROCESSED, f\"feats_64x32_with_times_{i}.npy\"))\n",
    "    label_np = np.load(os.path.join(CONFIG.PATH_PROCESSED, f\"labels_64x32_{i}.npy\"))\n",
    "    \n",
    "    data_list = data_np.tolist()\n",
    "    label_list = label_np.tolist()\n",
    "    \n",
    "    tmp_data_list = []\n",
    "    tmp_label_list = []\n",
    "    for idx in range(len(data_list)):\n",
    "        curdata = data_list[idx]\n",
    "        curlabel = label_list[idx]\n",
    "        if curlabel == 1:\n",
    "            for _ in range(enrich_ratio):\n",
    "                tmp_data_list.append(curdata)\n",
    "                tmp_label_list.append(curlabel)\n",
    "        else:\n",
    "            if idx % under_sampling_ratio == 0:\n",
    "                tmp_data_list.append(curdata)\n",
    "                tmp_label_list.append(curlabel)\n",
    "    cnt += len(label_list)   \n",
    "    error += sum(label_list)\n",
    "    \n",
    "    \n",
    "    data_list = tmp_data_list\n",
    "    label_list = tmp_label_list\n",
    "    \n",
    "    xgb_data_list.extend(data_list)\n",
    "    xgb_label_list.extend(label_list)\n",
    "    \n",
    "    data_np = np.array(data_list)\n",
    "    label_np = np.array(label_list)\n",
    "    \n",
    "    data_np = 2 / (1 + np.exp(-data_np)) - 1\n",
    "    \n",
    "    data = torch.tensor(data_np, dtype=torch.float32)\n",
    "    label = torch.tensor(label_np, dtype=torch.int64)\n",
    "     \n",
    "    dataset = TensorDataset(data, label)\n",
    "    dataset_list.append(dataset)\n",
    "\n",
    "datasets = torch.utils.data.ConcatDataset(dataset_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5934d3cc-6ee7-4a3b-9f44-e109ccf732b4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4296"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(xgb_data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6f422c94-ed4c-4bf8-9615-c3c0973c97ab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "566"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(xgb_label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ca6cce61-c310-4382-a82e-60184db6c00c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 64, 32])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1838638d-beec-4716-849f-bb4a63ab54cf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1, 64, 32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d674efc4-3177-40d5-be59-be3572dd004c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'len'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdatasets\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlen\u001b[49m()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'len'"
     ]
    }
   ],
   "source": [
    "datasets[0].len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "765656a5-d48a-4c09-bd48-9f439c759d60",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ret_data = None\n",
    "ret_label = None\n",
    "\n",
    "for i in range(31):\n",
    "    if i == 22:\n",
    "        continue\n",
    "    data_np = np.load(os.path.join(CONFIG.PATH_PROCESSED, f\"feats_64x32_with_times_{i}.npy\"))\n",
    "    label_np = np.load(os.path.join(CONFIG.PATH_PROCESSED, f\"labels_64x32_{i}.npy\"))\n",
    "    if ret_data is None:\n",
    "        ret_data = data_np\n",
    "        ret_label = label_np\n",
    "    else:\n",
    "        ret_data = np.concatenate((ret_data, data_np))\n",
    "        ret_label = np.concatenate((ret_label, label_np))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d690b940-248d-4ab2-9e94-e8a19b4cd5eb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30264, 1, 64, 32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6bab4444-db40-445d-9a16-d36833286964",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30264,)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "44aad297-117d-4d7e-85f7-8a84af4270b8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret_label[256]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950a219a-927b-4729-81f8-c00ad15fa6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ret_data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b860fc66-21c3-49eb-8976-31b05e398fde",
   "metadata": {
    "tags": []
   },
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf7ef29-f373-478a-a8fa-210afae690b6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## single testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "29839fcd-e477-445a-a184-011ec66dc076",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前设备: cuda\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [1, 64, 1, 64, 32]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 295\u001b[0m\n\u001b[1;32m    293\u001b[0m _inputs, _labels \u001b[38;5;241m=\u001b[39m _inputs\u001b[38;5;241m.\u001b[39mto(device), _labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    294\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 295\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_inputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# 在通道维度上增加1\u001b[39;00m\n\u001b[1;32m    296\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, _labels)\n\u001b[1;32m    297\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/anaconda3/envs/dram-py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[31], line 80\u001b[0m, in \u001b[0;36mCNNModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 80\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool(F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[1;32m     81\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool(F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x)))\n\u001b[1;32m     82\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool(F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv3(x)))\n",
      "File \u001b[0;32m~/anaconda3/envs/dram-py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/dram-py310/lib/python3.10/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/dram-py310/lib/python3.10/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [1, 64, 1, 64, 32]"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "# class CNNModel(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(CNNModel, self).__init__()\n",
    "#         self.channels = 64\n",
    "        \n",
    "#         self.conv1 = nn.Conv2d(1, self.channels, kernel_size=9, padding=4)\n",
    "#         self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "#         self.fc1 = nn.Linear(self.channels * 16 * 8, 128)  \n",
    "#         self.dropout = nn.Dropout(0.1)\n",
    "#         self.fc2 = nn.Linear(128, 2)  \n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.pool(F.relu(self.conv1(x)))\n",
    "#         x = x.view(-1, self.channels * 16 * 8)\n",
    "#         x = F.relu(self.fc1(x))\n",
    "#         x = self.dropout(x)\n",
    "#         x = self.fc2(x)\n",
    "#         return x\n",
    "\n",
    "# class CNNModel(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(CNNModel, self).__init__()\n",
    "#         self.channels1 = 32\n",
    "#         self.channels2 = 64\n",
    "        \n",
    "#         self.conv1 = nn.Conv2d(1, self.channels1, kernel_size=3, padding=1)\n",
    "#         self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "#         self.conv2 = nn.Conv2d(self.channels1, self.channels2, kernel_size=3, padding=1)\n",
    "#         self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "#         self.fc1 = nn.Linear(self.channels2 * 7 * 7, 128)\n",
    "        \n",
    "#         self.fc2 = nn.Linear(128, 2)  # 输出2类，根据你的任务调整输出维度\n",
    "\n",
    "#         self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.pool1(F.relu(self.conv1(x)))\n",
    "#         x = self.pool2(F.relu(self.conv2(x)))\n",
    "#         x = x.view(-1, self.channels2 * 7 * 7)\n",
    "#         x = F.relu(self.fc1(x))\n",
    "#         x = self.dropout(x)\n",
    "#         x = self.fc2(x)\n",
    "#         return x\n",
    "\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNModel, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=9, padding=4)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=7, padding=3)\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=5, padding=2)\n",
    "        self.conv4 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1)\n",
    "        \n",
    "        # self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1)\n",
    "        # self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        # self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
    "        # self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
    "        # self.conv4 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1)\n",
    "        \n",
    "        # 全连接层\n",
    "        self.fc1 = nn.Linear(256 * 2 * 1, 256) \n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.fc2 = nn.Linear(256, 2)  \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = self.pool(F.relu(self.conv4(x)))\n",
    "        x = x.view(-1, 256 * 2 * 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "########################################################################\n",
    "# 定义基本的ResNet块\n",
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=9, stride=stride, padding=4, bias=False)\n",
    "        # self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        # self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.downsample = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.downsample = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.conv1(x)\n",
    "        # out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        # out = self.bn2(out)\n",
    "        if self.downsample:\n",
    "            identity = self.downsample(x)\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "# 定义ResNet网络\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=2):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_channels = 64\n",
    "        self.conv1 = nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        # self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(256, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, out_channels, num_blocks, stride):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_channels, out_channels, stride))\n",
    "            self.in_channels = out_channels\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        # x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# 创建ResNet模型\n",
    "def ResNetCustom():\n",
    "    return ResNet(BasicBlock, [2, 2, 2])  # 这里定义了一个3层的ResNet\n",
    "########################################################################\n",
    "\n",
    "# wandb.init(\n",
    "#     project=\"dram\",\n",
    "    \n",
    "#     config={\n",
    "\n",
    "#     }\n",
    "# )\n",
    "    \n",
    "    \n",
    "\n",
    "batch_size = 64\n",
    "data_loader = DataLoader(datasets, shuffle=True)\n",
    "\n",
    "\n",
    "train_ratio = 0.8\n",
    "test_ratio = 1 - train_ratio  \n",
    "\n",
    "total_samples = len(data_loader.dataset)\n",
    "\n",
    "train_size = int(train_ratio * total_samples)\n",
    "test_size = total_samples - train_size\n",
    "\n",
    "indices = list(range(total_samples))\n",
    "\n",
    "random.shuffle(indices)\n",
    "\n",
    "train_indices = indices[:train_size]\n",
    "test_indices = indices[train_size:]\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "test_sampler = SubsetRandomSampler(test_indices)\n",
    "\n",
    "train_loader = DataLoader(datasets, batch_size=batch_size, sampler=train_sampler)\n",
    "test_loader = DataLoader(datasets, batch_size=batch_size, sampler=test_sampler)\n",
    "\n",
    "\n",
    "############################################################################\n",
    "\n",
    "\n",
    "\n",
    "#######################################################################################\n",
    "\n",
    "# # model = CNNModel()\n",
    "# model = ResNetCustom()\n",
    "\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# print(\"当前设备:\", device)\n",
    "# model.to(device)\n",
    "\n",
    "# criterion = nn.CrossEntropyLoss() \n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.001) \n",
    "\n",
    "# num_epochs = 100\n",
    "# for epoch in range(num_epochs):\n",
    "#     running_loss = 0.0\n",
    "#     for _inputs, _labels in train_loader:\n",
    "#         _inputs, _labels = _inputs.to(device), _labels.to(device)\n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = model(_inputs.unsqueeze(1))  # 在通道维度上增加1\n",
    "#         loss = criterion(outputs, _labels)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         running_loss += loss.item()\n",
    "#         # wandb.log({\"loss\": running_loss})\n",
    "#     print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {running_loss / len(data_loader)}')\n",
    "\n",
    "# print('Finished Training')\n",
    "\n",
    "\n",
    "# model.eval()\n",
    "\n",
    "\n",
    "\n",
    "# predictions = []\n",
    "# true_labels = []\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     for _inputs, _labels in train_loader:\n",
    "#         _inputs, _labels = _inputs.to(device), _labels.to(device)\n",
    "#         outputs = model(_inputs.unsqueeze(1))\n",
    "#         _, predicted = torch.max(outputs.data, 1)\n",
    "#         predictions.extend(predicted.tolist())\n",
    "#         true_labels.extend(_labels.tolist())\n",
    "\n",
    "# precision = precision_score(true_labels, predictions)\n",
    "# recall = recall_score(true_labels, predictions)\n",
    "\n",
    "# print(f\"all: {len(true_labels)}, error_labels: {sum(true_labels)}\")\n",
    "# print(f'Precision: {precision}')\n",
    "# print(f'Recall: {recall}')\n",
    "\n",
    "# # wandb.log({\"train precision\": precision, \"train recall\": recall})\n",
    "\n",
    "\n",
    "# predictions = []\n",
    "# true_labels = []\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     for _inputs, _labels in test_loader:\n",
    "#         _inputs, _labels = _inputs.to(device), _labels.to(device)\n",
    "#         outputs = model(_inputs.unsqueeze(1))\n",
    "#         _, predicted = torch.max(outputs.data, 1)\n",
    "#         predictions.extend(predicted.tolist())\n",
    "#         true_labels.extend(_labels.tolist())\n",
    "\n",
    "# print(f\"all: {len(true_labels)}, error_labels: {sum(true_labels)}\")\n",
    "        \n",
    "# precision = precision_score(true_labels, predictions)\n",
    "# recall = recall_score(true_labels, predictions)\n",
    "\n",
    "# print(f'Precision: {precision}')\n",
    "# print(f'Recall: {recall}')\n",
    "\n",
    "# # wandb.log({\"test precision\": precision, \"test recall\": recall})\n",
    "\n",
    "# # wandb.finish()\n",
    "#######################################################################################\n",
    "\n",
    "\n",
    "model = CNNModel()\n",
    "# wandb.watch(model)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"当前设备:\", device)\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss() \n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001) \n",
    "\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for _inputs, _labels in train_loader:\n",
    "        _inputs, _labels = _inputs.to(device), _labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(_inputs.unsqueeze(0))  # 在通道维度上增加1\n",
    "        loss = criterion(outputs, _labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        # wandb.log({\"loss\": running_loss})\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {running_loss / len(data_loader)}')\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "\n",
    "model.eval()\n",
    "\n",
    "\n",
    "\n",
    "predictions = []\n",
    "true_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for _inputs, _labels in train_loader:\n",
    "        _inputs, _labels = _inputs.to(device), _labels.to(device)\n",
    "        outputs = model(_inputs.unsqueeze(0))\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        predictions.extend(predicted.tolist())\n",
    "        true_labels.extend(_labels.tolist())\n",
    "\n",
    "precision = precision_score(true_labels, predictions)\n",
    "recall = recall_score(true_labels, predictions)\n",
    "\n",
    "print(f\"all: {len(true_labels)}, error_labels: {sum(true_labels)}\")\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall: {recall}')\n",
    "\n",
    "# wandb.log({\"train precision\": precision, \"train recall\": recall})\n",
    "\n",
    "\n",
    "predictions = []\n",
    "true_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for _inputs, _labels in test_loader:\n",
    "        _inputs, _labels = _inputs.to(device), _labels.to(device)\n",
    "        outputs = model(_inputs.unsqueeze(0))\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        predictions.extend(predicted.tolist())\n",
    "        true_labels.extend(_labels.tolist())\n",
    "\n",
    "print(f\"all: {len(true_labels)}, error_labels: {sum(true_labels)}\")\n",
    "        \n",
    "precision = precision_score(true_labels, predictions)\n",
    "recall = recall_score(true_labels, predictions)\n",
    "\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall: {recall}')\n",
    "\n",
    "# wandb.log({\"test precision\": precision, \"test recall\": recall})\n",
    "\n",
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b98399-9f88-4aff-a7ea-162f0a95a679",
   "metadata": {
    "tags": []
   },
   "source": [
    "## lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "10a4d39a-362d-4b9e-967d-53f0073d89f1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 447, number of negative: 2989\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006058 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5459\n",
      "[LightGBM] [Info] Number of data points in the train set: 3436, number of used features: 506\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.130093 -> initscore=-1.900136\n",
      "[LightGBM] [Info] Start training from score -1.900136\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Accuracy: 81.51%\n",
      "Precision: 0.3684210526315789\n",
      "Recall: 0.47058823529411764\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 假设你有N个样本，每个样本是一个3x2的矩阵\n",
    "# N = 1000\n",
    "data = np.array(xgb_data_list)\n",
    "labels = np.array(xgb_label_list)\n",
    "\n",
    "# 将数据整形为(N, 6)，以便传递给LightGBM\n",
    "data = data.reshape(len(data), -1)\n",
    "\n",
    "# 划分训练集和测试集\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# 创建LightGBM数据集\n",
    "train_data = lgb.Dataset(X_train, label=y_train)\n",
    "test_data = lgb.Dataset(X_test, label=y_test, reference=train_data)\n",
    "\n",
    "# 定义LightGBM参数\n",
    "params = {\n",
    "    'objective': 'binary',  # 二分类问题\n",
    "    'metric': 'binary_error',  # 评估指标为二分类错误率\n",
    "    'boosting_type': 'gbdt',  # 使用梯度提升树\n",
    "    'num_leaves': 128,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.3\n",
    "}\n",
    "\n",
    "# 训练LightGBM模型\n",
    "num_round = 100  # 迭代次数\n",
    "bst = lgb.train(params, train_data, num_round, valid_sets=[test_data])\n",
    "\n",
    "# 预测\n",
    "y_pred = bst.predict(X_test, num_iteration=bst.best_iteration)\n",
    "y_pred_binary = [1 if x >= 0.2 else 0 for x in y_pred]\n",
    "\n",
    "# 评估模型\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred_binary)\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
    "\n",
    "precision = precision_score(y_test, y_pred_binary)\n",
    "recall = recall_score(y_test, y_pred_binary)\n",
    "\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall: {recall}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e215bd95-43c6-4614-aa94-e4c639ba4fbe",
   "metadata": {},
   "source": [
    "## pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "87e173cf-e4aa-444b-9bca-316af168e64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ptn_data = np.load(os.path.join(CONFIG.PATH_PROCESSED, f\"feats_with_times_{0}.npy\"))\n",
    "ptn_label = np.load(os.path.join(CONFIG.PATH_PROCESSED, f\"labels_{0}.npy\"))\n",
    "\n",
    "ptn_data = 2/(1+np.exp(-ptn_data))-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "bb01f190-4216-41f6-9f2e-cc7897404a6e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ptn_data[45]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39680c0-60ef-496c-a69f-322d135c5d3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c542d5e-8e7b-4bf8-ad72-991f8c4d1752",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9ee474ae-5f91-4c5f-bebd-cf39d7b2b6ee",
   "metadata": {},
   "source": [
    "## xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "b3b9c888-a66f-47e5-973e-8b0745e3ea56",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 85.12%\n",
      "Precision: 0.40816326530612246\n",
      "Recall: 0.16806722689075632\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 假设你有N个样本，每个样本是一个3x2的矩阵\n",
    "# N = 1000\n",
    "data = np.array(xgb_data_list)\n",
    "labels = np.array(xgb_label_list)\n",
    "\n",
    "# 将数据整形为(N, 6)，以便传递给LightGBM\n",
    "data = data.reshape(len(data), -1)\n",
    "\n",
    "# 划分训练集和测试集\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# model = xgb.XGBClassifier(max_depth=6,\n",
    "#     learning_rate=0.01,\n",
    "#     n_estimators=100,\n",
    "#     objective='binary:logistic',\n",
    "#     eval_metric='logloss',\n",
    "#     random_state=42)\n",
    "model = xgb.XGBClassifier(objective='binary:logistic')\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# 评估模型\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
    "\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall: {recall}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c4557c-4a99-4dd0-a46f-e6532163abc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d884d05c-99f0-4db1-a58a-75fc7ebe0ce9",
   "metadata": {},
   "source": [
    "## Random Forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "e23408d6-8daf-4c17-bc18-c91c9af4e3b6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 84.42%\n",
      "Precision: 0.3972602739726027\n",
      "Recall: 0.24369747899159663\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "data = np.array(xgb_data_list)\n",
    "labels = np.array(xgb_label_list)\n",
    "\n",
    "data = data.reshape(len(data), -1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=50, \n",
    "                            random_state=42,\n",
    "                            criterion=\"gini\")\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
    "\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall: {recall}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9782c7-c672-43c3-86a1-2277f84d4f1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a259ab01-c092-44e8-b48c-067730f987d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e00c1ab-26ab-49f6-909f-c9af2ffb36ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc67780-02d3-4850-ac92-3c5f686e97b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d6333bf5-c14c-42f1-9c7b-2b73f59bd78e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "d0fedff1-1192-4535-9693-4219da44fdf0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练准确率: 0.51\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 创建示例数据（请根据您的数据替换这部分代码）\n",
    "N = 1000\n",
    "data = np.random.rand(N, 1, 32, 16)\n",
    "labels = np.random.randint(0, 2, N)\n",
    "\n",
    "# 划分训练集和测试集\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# 将数据转换为 PyTorch 张量\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "# 创建数据加载器\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# 定义基本的ResNet块\n",
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.downsample = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.downsample = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        if self.downsample:\n",
    "            identity = self.downsample(x)\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "# 定义ResNet网络\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=2):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_channels = 64\n",
    "        self.conv1 = nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(256, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, out_channels, num_blocks, stride):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_channels, out_channels, stride))\n",
    "            self.in_channels = out_channels\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# 创建ResNet模型\n",
    "def ResNetCustom():\n",
    "    return ResNet(BasicBlock, [2, 2, 2])  # 这里定义了一个3层的ResNet\n",
    "\n",
    "# 训练模型\n",
    "model = ResNetCustom()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# 模型评估\n",
    "model.eval()\n",
    "y_pred = []\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        y_pred.extend(predicted.cpu().numpy())\n",
    "\n",
    "accuracy = accuracy_score(y_train, y_pred)\n",
    "print(f\"训练准确率: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c80a0bab-4be2-4f43-8cb6-225222a1c595",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.7720351815223694\n",
      "Epoch 2/10, Loss: 0.6970367385790899\n",
      "Epoch 3/10, Loss: 0.6913520051882818\n",
      "Epoch 4/10, Loss: 0.6891163358321557\n",
      "Epoch 5/10, Loss: 0.6859682110639719\n",
      "Epoch 6/10, Loss: 0.681311194713299\n",
      "Epoch 7/10, Loss: 0.672152775984544\n",
      "Epoch 8/10, Loss: 0.662941758449261\n",
      "Epoch 9/10, Loss: 0.641889397914593\n",
      "Epoch 10/10, Loss: 0.6274937574680035\n",
      "Accuracy on test set: 47.00%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "\n",
    "# 生成模拟数据\n",
    "N = 1000\n",
    "data = np.random.randn(N, 1, 64, 32).astype(np.float32)\n",
    "labels = np.random.randint(0, 2, N)  # 生成随机标签\n",
    "\n",
    "# 划分训练集和测试集\n",
    "split_ratio = 0.8\n",
    "split = int(N * split_ratio)\n",
    "train_data, test_data = data[:split], data[split:]\n",
    "train_labels, test_labels = labels[:split], labels[split:]\n",
    "\n",
    "# 创建自定义数据集类\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = {'data': self.data[idx], 'label': self.labels[idx]}\n",
    "        return sample\n",
    "\n",
    "# 创建数据集和数据加载器\n",
    "batch_size = 64\n",
    "train_dataset = CustomDataset(train_data, train_labels)\n",
    "test_dataset = CustomDataset(test_data, test_labels)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# 定义一个两层CNN模型\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc = nn.Linear(32 * 16 * 8, 2)  # 输出2个类别\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# 创建模型实例\n",
    "model = SimpleCNN()\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 训练模型\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for batch in train_loader:\n",
    "        inputs, labels = batch['data'], batch['label']\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {running_loss / len(train_loader)}')\n",
    "\n",
    "# 在测试集上验证模型\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        inputs, labels = batch['data'], batch['label']\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy on test set: {(correct / total) * 100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbba72ca-6add-4a9f-bdfa-4ee7c9eaba18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dram-py310",
   "language": "python",
   "name": "dram-py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
